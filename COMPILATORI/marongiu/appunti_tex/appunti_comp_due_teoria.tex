\input{../../../preamble}

\title{%
  Compilatori\\
\large Parte Due}

\begin{document}
\maketitle
\tableofcontents
\newpage~
\newpage
\section{Introduzione}

\subsection{Motivazione}

Ricordiamo il ruolo del compilatore tra le tecnologie informatiche, quello dell'ISA e del linguaggio assembly, i passaggi gestiti dal compilatore, dall'assembler, eccetera
\begin{itemize}
  \item Il compilatore \textbf{traduce un programma sorgente in linguaggio macchina}
  \item L'ISA agisce da "interfaccia" tra HW e SW (fornisce a SW il set di istruzioni, e specifica a HW che cosa fanno)
\end{itemize}

\subsubsection{La funzione dei compilatori}

\begin{itemize}
  \item Funzione principale e pi\`u nota: trasformare il codice \textbf{da un linguaggio ad un altro} (es. C $\rightarrow$ Assembly RISC-V) (ricordiamo che \`e solo il primo passo di un'intera toolchain di programmi per creare eseguibili)

  \item Gestendo la traduzione a linguaggio macchina al posto dei programmatori, l'altra funzione importante \`e l'\textbf{ottimizzazione} del codice, che permette la \textbf{produzione di eseguibili di stesse funzionalit\`a}, ma diversi a livello di \textbf{dimensioni} (es. per sistemi embedded e high-performance), \textbf{consumo energetico}, \textbf{velocit\`a di esecuzione}, ma anche in termini di determinate \textbf{caratteristiche architetturali} utilizzate (es. proc.~multicore)
\end{itemize}

\subsubsection{L'evoluzione dei compilatori}

Le rivoluzioni in termini di "classe" di dispositivi e di dimensioni dei transistor sono molto frequenti (Bell, Moore), e nei primi 2000 si arriva ai \textbf{limiti fisici della miniaturizzazione e della frequenza} operativa dei processori (problemi di dissipazione del calore) $\rightarrow$ idea di cambiare il paradigma di sviluppo di un processore: dal singolo core sempre pi\`u potente passo a \textbf{pi\`u core "isopotenti"} sullo stesso chip
\begin{wrapfigure}{l}{.3\textwidth}
  \centering
  \includegraphics[width=.25\textwidth]{intro_1.png}
\end{wrapfigure}

\noindent$\sim$ 2005: plateau di consumo, frequenza e performance di programmi \textit{sequenziali}, aumento di performance di p.~che \textbf{sfruttano la parallelizzazione} $\rightarrow$ i programmi devono essere "consapevoli" che il processore \`e multicore!\\
Il compilatore mantiene un ruolo fondamentale: oltre a rendere meno "traumatico" il passaggio alla programmazione parallela, (non sono ancora auto-parallelizzanti) si interfaccia con i nuovi paradigmi di programmazione parallela offerti ai programmatori: il programmatore sfrutta interfacce semplici e astratte, mentre il compilatore traduce i costrutti in codice parallelo eseguibile (es. OpenMP)

\subsubsection{Eterogeneit\`a architetturale}

La programmazione parallela e il parallelismo architetturale sono oggi paradigmi consolidati, e i processori general purpose (seppur multicore e ottimizzati) non sono sufficienti per attivit\`a specializzate come la grafica $\rightarrow$ nascono componenti \textbf{acceleratori} di vario tipo: GPU, GPGPU, FPGA, TPU, NPU...
Questo complica ulteriormente la scrittura del software, e dunque impone altre evoluzioni nei compilatori e nelle ottimizzazioni.

\subsection{Ottimizzazione}

Ricordiamo le metriche usate:

\noindent\begin{minipage}[c]{.5\textwidth}
  \begin{equation*}
    \text{Performance} = {{1}\over\text{Execution Time}}
  \end{equation*}
\end{minipage}
\begin{minipage}[c]{.5\textwidth}
  \begin{equation*}
    \text{Execution Time} = {\textcolor{blue}{\text{Instruction Count}} \times \textcolor{red}{\text{CPI}} \over \textcolor{red}{\text{Frequency}}}
  \end{equation*}
\end{minipage}\\

Le ottimizzazioni possono avvenire dal punto di vista \textcolor{red}{HW (parametri architetturali)} e da quello \textcolor{blue}{SW (p.~di programma)}. Il compilatore pu\`o agire anche ad es. a livello di cache, aiutando a ridurre i miss e dunque i CPI delle istruzioni \lstinline|load| e \lstinline|store| (sappiamo che il costo di accesso aumenta di ordini di grandezza)

\subsubsection{Esempi di ottimizzazione}

\begin{emphasize}
  Distinguiamo le ottimizzazioni che avvengono a compile time o a runtime (statiche o dinamiche)
\end{emphasize}

\begin{itemize}
  \item \textbf{AS (Algebraic Semplification)}: ottimizzazione a runtime
    \begin{lstlisting}
-(-i); $\rightarrow$ i;
b or true; $\rightarrow$ true; //cortocircuito logico\end{lstlisting}
  \item \textbf{CF (Constant Folding)}:  valutare ed espandere espressioni costanti a compile time
    \begin{lstlisting}
c = 1+3; $\rightarrow$ c = 4;
(100<0) $\rightarrow$ false\end{lstlisting}

\item \textbf{SR (Strength Reduction)}: sostituisco op. costose con altre pi\`u semplici: classico es. \lstinline|MUL| rimpiazzate da \lstinline|ADD/SHIFT| (esecuzione in 1 ciclo invece di multic.):\vspace{.5em}\\
    \begin{minipage}[c]{.4\textwidth}
      \begin{lstlisting}
y = x*2;
y = x * 17;\end{lstlisting}
    \end{minipage}
    \hfill
    $\rightarrow$
    \hfill
    \begin{minipage}[c]{.4\textwidth}
      \begin{lstlisting}
y = x+x;
y = (x<<4) + x;\end{lstlisting}
    \end{minipage}\\
    es.~sofisticato: \lstinline|for| con operazioni su array, sostituito da operazioni su puntatori (aritmetica dei pt.) $\rightarrow$ il risultato si vede nel codice assembly\\
    \begin{minipage}[c]{.4\textwidth}
      \begin{lstlisting}
for (i=0; i<100; i++)
  a[i] = i*100;\end{lstlisting} 
      \end{minipage}
      \hfill
      $\rightarrow$
      \hfill
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
t = 0;
for (; t<10000; t += 100) {
  *a = t;
  a = a + 4;
}\end{lstlisting}
    \end{minipage}

    \begin{minipage}[c]{.4\textwidth}
      \begin{lstlisting}
li s0, 0 // i = 0
li s1, 100
LOOP:
bge s0, s1, EXIT
slli s2, s0, 2
add s2, s2, a0
mul s3, s0, 100
sw s3, 0(s2)
addi s0, s0, 1
jal zero, LOOP
EXIT:\end{lstlisting} 
      \end{minipage}
      \hfill
      $\rightarrow$
      \hfill
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
li s0, 0 // t = 0
li s1, 10000
LOOP:
bge s0, s1, EXIT
sw s0, 0(a0)
addi a0, a0, 4
jal zero, LOOP
EXIT:\end{lstlisting}
      \end{minipage}

    \item \textbf{CSE (Common Subexpression Elimination)}: elimino i calcoli ridondanti di una stessa espressione riutilizzata in pi\`u istruzioni (statement)\\
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
y = b * c + 4
z = b * c - 1\end{lstlisting}
      \end{minipage}
      \hfill $\rightarrow$ \hfill
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
x = b * c
y = x + 4
z = x - 1\end{lstlisting}
      \end{minipage}
    \item \textbf{DCE (Dead Code Elimination)}: elimino tutte le istruzioni che producono codice mai letto (e dunque utilizzato), es. variabili assegnate e mai lette, codice irraggiungibile $\rightarrow$ uno dei passi eseguiti pi\`u di frequente durante l'ottimizzazione del codice da parte del compilatore, per rimuovere anche tutto il dead code generato dagli altri passi di ottimizzazione\vspace{.5em}\\
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
b = 3
c = 1 + 3
d = 3 + c\end{lstlisting}
      \end{minipage}
      \hfill $\rightarrow$ \hfill
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
c = 1 + 3
d = 3 + c\end{lstlisting}
      \end{minipage}

      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
if (100<0)
  {a = 5}\end{lstlisting}
      \end{minipage}
      \hfill $\rightarrow$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
if (false)
  {}\end{lstlisting}
      \end{minipage}
      \hfill $\rightarrow$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}

        \end{lstlisting}
      \end{minipage}
    \item \textbf{Copy Propagation}: per uno statement \lstinline|x = y|, sostituisco gli usi futuri di \lstinline|x| con \lstinline|y| se non sono cambiati nel frattempo (propedeutico alla DCE)\vspace{.5em}\\
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
x = y;
c = 1 + x;
d = y + c;\end{lstlisting}
      \end{minipage}
      \hfill $\rightarrow$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
x = y;
c = 1 + y;
d = y + c;\end{lstlisting}
      \end{minipage}
      \hfill $\tiny\underrightarrow{\text{DCE}}$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
c = 1 + y;
d = y + c;\end{lstlisting}
      \end{minipage}
    \item \textbf{CP (Constant Propagation)}: sostituisco usi futuri di una variabile con assegnato valore costante con la costante stessa (se la variabile non cambia) (sempre ipotesi che i valori a fine es.~siano poi \textbf{usati}, e non dead code)\vspace{.5em}\\
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
b = 3;
c = 1 + b;
d = b + c;\end{lstlisting}
      \end{minipage}
      \hfill $\tiny\underrightarrow{\text{CP}}$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
b = 3;
c = 1 + 3;
d = 3 + c;\end{lstlisting}
      \end{minipage}
      \hfill $\tiny\underrightarrow{\text{CF}}$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
b = 3;
c = 4;
d = 3 + c;\end{lstlisting}
      \end{minipage}
      \hfill $\tiny\underrightarrow{\text{CP}}$ \hfill

      \hfill $\tiny\underrightarrow{\text{CP}}$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
b = 3;
c = 4;
d = 3 + 4;\end{lstlisting}
      \end{minipage}
      \hfill $\tiny\underrightarrow{\text{CF}}$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
b = 3;
c = 4;
d = 7;\end{lstlisting}
      \end{minipage}
      \hfill $\tiny\underrightarrow{\text{DCE}}$ \hfill
      \begin{minipage}[c]{.25\textwidth}
        \begin{lstlisting}
d = 7;\end{lstlisting}
      \end{minipage}
    \item \textbf{LICM (Loop Invariant Code Motion)}: si occupa di muovere fuori dai loop tutto il codice \textbf{loop invariant}; evita i calcoli ridondanti\\
      \begin{minipage}[c]{.4\textwidth}
        \begin{lstlisting}
while (i < 100) {
  *p = x / y + i;
  i = i + 1;
}\end{lstlisting}
    \end{minipage}
    \hfill $\rightarrow$ \hfill
    \begin{minipage}[c]{.4\textwidth}
      \begin{lstlisting}
t = x / y;
while (i < 100) {
  *p = t + i;
  i = i + 1;
}\end{lstlisting}
  \end{minipage}
\end{itemize}

\subsubsection{Ottimizzazioni sui loop}

\begin{itemize}
  \item grande impatto sulla performance dell'intero programma (per ovvie ragioni)
  \item spesso sono ottimizzazioni propedeutiche a quelle machine-specific (effettuate nel backend): register allocation, instruction level parallelism, data parallelism, data-cache locality
\end{itemize}

\subsection{Anatomia di un compilatore}

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{intro_3.png}
\end{figure}

\begin{itemize}
  \item almeno due compiti: \textbf{analisi del sorgente} e \textbf{sintesi di un programma in linguaggio macchina}, operando su una IR che si interpone tra frontend e backend, e tra source code e target code
  \item Il blocco di middle-end agisce su IR, e in vari passaggi lo trasforma e ottimizza ($\neq$ a seconda del compilatore)
  \item caso llvm: \lstinline|clang| (frontend) $\rightarrow$ \lstinline|opt| (middleend) $\rightarrow$ \lstinline|llc| (backend)
  \item \lstinline|opt| si basa su una serie di \textbf{passi di ottimizzazione (o di analisi)}: un passo di analisi scorre l'IR e lo analizza (non lo trasforma, ma produce informazioni utili); un passo di ottimizzazione sfrutta informazioni conosciute per trasformare l'IR (applica le ottimizzazione)
  \item alcune ottimizzazioni non possono essere effettuate o finalizzate senza conoscere l'architettura target (es. sulle cache), e dunque vengono eseguite dal backend
\end{itemize}

\subsubsection{Flag di ottimizzazione}

sono flag che passo al compilatore (al pass manager) per influenzare \textbf{ordine e numero dei passi di ottimizzazione}
\begin{multicols}{2}
  \begin{itemize}
    \item \lstinline|-g|: solo debugging, nessuna ottimizzazione
    \item \lstinline|-O0|: nessuna ottimizzazione
    \item \lstinline|-O1|: solo ott. semplici
    \item \lstinline|-O2|: ott. pi\`u aggressive
    \item \lstinline|-O3|: ordine dei passi che sfrutta compromessi tra velocit\`a e spazio occupato
    \item \lstinline|-OS|: ottimizza per dimensione del compilato
  \end{itemize}
\end{multicols}


\subsubsection{Uso di IR}

un backend che fa uso di IR permette di disaccoppiare con facilit\`a frontend e backend, lavorare su ottimizzazioni machine-independent, semplificare il supporto per molti linguaggi, eccetera

\begin{emphasize}
  Per supportare un nuovo linguaggio o una nuova architettura, basta scrivere un nuovo front/backend - il middle-end pu\`o rimanere lo stesso!
\end{emphasize}

\subsubsection{Ingredienti dell'ottimizzazione}

\begin{itemize}
  \item \textbf{formulare un problema di ottimizzazione} con molti casi di applicazione, sufficientemente efficiente e impattante su parti significative
  \item[$\rightarrow$] \textbf{rappresentazione} che astrae dettagli rilevanti $\rightarrow$ \textbf{analisi} di applicabilit\`a $\rightarrow$ \textbf{trasformazione del codice} $\rightarrow$ \textbf{testing} $\rightarrow \, \circlearrowleft$
\end{itemize}

\vspace{-2em}
\section{Rappresentazione intermedia}

Ricordiamo: middle end come sequenza di passi, di analisi o di trasformazione $\rightarrow$ per analizzare e trasformare il codice occorre una rappr.~intermedia (IR) \textbf{espressiva} che \textbf{mantenga le informazioni importanti da un passo all'altro}

\vspace{-1em}
\subsection{Propriet\`a di una IR}

scegliamo IR diverse a seconda del loro uso, in generale alcune caratteristiche sono sempre richieste:
\begin{itemize}
  \item facilit\`a di \textbf{generazione} (effetti sul frontend)
  \item facilit\`a e costo di \textbf{manipolazione}
  \item livello di astrazione e di \textbf{dettaglio esposto}: effetti su frontend e backend ($\neq$ IR da un lato e dall'altro, a seconda di astraz.~e dettaglio necessari)
\end{itemize}

\vspace{-1em}
\subsection{Tipi di IR}

\begin{itemize}
  \item AST (abstract syntax tree)
  \item DAG (grafi diretti aciclici)
  \item 3AC (3-address code): simile all'assembly (3 indirizzi: registro destinazione e max 2 operandi)
  \item SSA (Static Single Assignment): evoluzione di 3ac con ulteriori propriet\`a di control flow
  \item CFG (control flow graphs): rappresenta "come" vengono chiamate le funzioni (a partire dal main)
  \item CG (call graph)
  \item PDG (program dependence graphs): fondamentale per lavorare sul parallelismo, multithreading...
\end{itemize}

\vspace{-1em}
\begin{emphasize}
  Le ott.~inter-procedurali devono per forza basarsi su IR di tipo CG (es. per decidere quando fare \textbf{inlining} - espandere il codice della funzione invece di chiamarla - evidente tradeoff tra dimensione del codice e overhead dovuto alla chiamata di funzione)
\end{emphasize}

\subsection{Categorie di IR}

\begin{itemize}
  \item grafiche (o strutturali)
    \begin{itemize}
      \item orientate ai grafi
      \item molto usate nella source-to-source translation, tipicam.~per ott.~che non hanno bisogno della struttura sofisticata di un middle-end\\
        es.~openMP: di fatto annotazioni sul codice, come strumento semplice per la parall.~(es. \textbf{outlining}: prendo es un loop e lo impacchetto in una funzione che poi dovra essere eseguita dai thread per la parallelizzazione) - non sto ottimizzando nel senso proprio del termine, ma sto trasformando il codice e lo sto rendendo eseguibile in maniera parallela
      \item solitamente voluminose (basate su grafi) - tradeoff con il fatto che non coinvolgono il middle-end
      \item es.~AST, DAG
    \end{itemize}
  \item lineari
    \begin{itemize}
      \item pseudocodice per macchine astratte
      \item livello di astrazione vario
      \item strutture dati semplici e compatte
      \item facile da riarrangiare (evidentemente il pi\`u comodo per eseguire le ottimizzazioni)
      \item es. 3AC
    \end{itemize}
  \item ibride (sfruttano combinazioni delle prime due) (es.~CFG)
\end{itemize}
\subsection{Esempi di rappresentazione}


\subsubsection{Sintassi concreta (testo)}

Pi\`u semplice in quanto pi\`u vicina al livello di astrazione "umano" di ragionamento sul programma, ma non il livello corretto per ottimizzare ne comprendere correttamente la semantica del programma

\begin{lstlisting}[language=java]
let value = 8;
let result = 1;
for (let i = value; i>0; i = i - 1) {
  result = result * i;
}
console.log(result);\end{lstlisting}

\vspace{-.5em}
\subsubsection{AST (Abstract Syntax Tree)}

Albero i cui nodi rappresentano diverse parti del programma: il nodo radice rappresenta il \textbf{programma}, il quale a sua volta contiene un blocco di istruzioni dal quale discendono tanti figli quante le sue istruzioni

\vspace{1em}
\noindent\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{ir_1.png}
\end{minipage}\hfill
\begin{minipage}[c]{.67\textwidth}
\begin{lstlisting}[linewidth=.5\linewidth]
x = a + a * (b - c)
y = (b - c ) * d
z = x + y\end{lstlisting}
\textbf{PRO}: molto comodo per interpreti (basta usare una fz.~ricorsiva per processare l'albero)

\textbf{CONTRO}: un nodo \`e un oggetto troppo generico $\rightarrow$ analizzare un ast per l'ottimizzazione impone ogni volta di ragionare sulla differenza semantica tra i nodi (complica molto)
\end{minipage}


\vspace{-.5em}
\subsubsection{DAG (Directed Acyclic Graph)}

Contrazione di ast che evita la duplicazione di espressioni $\rightarrow$ \textbf{rappresentazione pi\`u compatta}

\noindent\textbf{Limite}: il riuso e possibile solamente dimostrando che il suo \textbf{valore non cambia} nel programma
\begin{emphasize}
  essendo assegnamenti e chiamate frequentissimi, il fatto che il dag non abbia nozione di come le espr.~cambino valore nel tempo non lo rende un buon candidato per le ottimizzazioni
\end{emphasize}

\begin{example}[frametitle={Esempi}]
  \noindent\begin{minipage}[c]{.18\textwidth}
    \includegraphics[width=\textwidth]{ir_2.png}
  \end{minipage}
  \begin{minipage}[c]{.23\textwidth}
    \begin{lstlisting}
x = a+a*(b-c);
y = (b-c)*d;
z = x+y;
# espr. trovate
t1 = b-c;
t2 = a*t1;
x = a+t2;
y = t1*d;
z = x+y;\end{lstlisting}
  \end{minipage}\hfill\vline\hfill
  \begin{minipage}[c]{.18\textwidth}
    \includegraphics[width=\textwidth]{ir_3.png}
  \end{minipage}
  \begin{minipage}[c]{.35\textwidth}
    \begin{lstlisting}
a = b + c;
b = a - d;
c = b + c;
d = a - d;
# espr. trovate (ERRORE)
a = b + c; # cambia val.
d = a - d;
c = d + c;\end{lstlisting}
  \end{minipage}
\end{example}



\subsubsection{3AC (3-Address Code)}

\begin{minipage}[c]{.65\textwidth}
  Evidentemente adatto: tutte le istr.~del programma vengono spezzettate in istr.~di forma semplice simile all'assembly, di tipo \lstinline|x = y op z| (1 operatore, massimo 3 operandi)

  \noindent\hfill
  \begin{minipage}[c]{.3\textwidth}
    \begin{lstlisting}
x - 2 * y\end{lstlisting}
  \end{minipage}\hfill
  $\rightarrow$\hfill
  \begin{minipage}[c]{.32\textwidth}
    \begin{lstlisting}
t1 = 2 * y
t2 = x - t1\end{lstlisting}
  \end{minipage}\hfill
\end{minipage}
\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{ir_4.png}
\end{minipage}

\textbf{PRO}:
\begin{itemize}
  \item espressioni complesse spezzettate
  \item forma compatta e simil-assembly
  \item registri temporanei \textbf{intermedi, virtuali e illimitati} (tralascio problemi architetturali - n.~di r.~fisici a disposizione e eventuali op. di spill, cio\`e aggiungere load o store in mancanza di r.~fisici)
\end{itemize}

\paragraph{Varianti di 3AC}~\\

A seconda dei vincoli che ho per l'implementazione pratica:\\

\noindent\begin{minipage}[c]{.7\textwidth}
  \begin{itemize}
    \item \textbf{quadruple}: id istruzione, opcode, i 3 registri $\rightarrow$ semplice struttura record, facile da analizzare e riordinare ma i nomi espliciti prendono pi\`u spazio
    \item \textbf{triple}: id istruzione, opcode, 2 operandi $\rightarrow$ uso l'indice dell'espressione nell'array come "nome" del registro destinazione $\rightarrow$ risparmio spazio, ma diventa piu complesso da analizzare (nomi impliciti) e riordinare
  \end{itemize}
  \begin{emphasize}[frametitle={Inapplicabilit\`a diretta della Constant Propagation con forma 3AC}]
    La CP \`e applicabile solo se la variabile \textbf{se non cambia nel frattempo} $\rightarrow$ una IR di tipo 3AC non pu\`o applicarla immediatamente (devo prima analizzare il resto del codice)
  \end{emphasize}
\end{minipage}
\begin{minipage}[c]{.3\textwidth}
  \centering
  \includegraphics[width=.7\textwidth]{ir_5.png}
  \includegraphics[width=.7\textwidth]{ir_6.png}
\end{minipage}

\subsubsection{SSA (Static Single Assignment)}

\begin{itemize}
  \item Evoluzione di 3AC che impone che la \textbf{definizione (assegnamento)} delle variabili avvenga \textbf{solo una volta} (def.~multiple sono tradotte in multiple versioni della var)
  \item \textbf{PRO}: ogni definizione ha associata direttamente una \textbf{lista di tutti i suoi usi} - semplifica enormemente le ottimizzazioni di tipo CP e non solo
\end{itemize}


\begin{emphasize}
  Quasi sempre uno dei passi di ottimizzazione prevede il passaggio a forma SSA
\end{emphasize}

\begin{emphasize}
  La scelta della IR dipende ovviamente dal livello di dettaglio necessario per ogni specifico compito $\rightarrow$ \textbf{in un compilatore coesistono pi\`u IR} (anche per questo esistono forme ibride)
\end{emphasize}

\subsubsection{CFG (Control Flow Graph)}

\noindent\begin{minipage}[c]{.65\textwidth}
  \begin{itemize}
    \item modella il trasferimento (flusso) del controllo in un programma tra \textbf{blocchi} di istruzioni
    \item permette di aggiungere informazioni sui \textbf{salti} al di sopra di una IR lineare
    \item i suoi nodi sono Basic Block
    \item gli archi rappresentano il flusso di controllo del programma (loop, condizioni, ecc.)
  \end{itemize}
\end{minipage}
\begin{minipage}[c]{.35\textwidth}
  \includegraphics[width=\textwidth]{ir_7.png}
\end{minipage}

{%
  \begin{wrapfigure}{r}{.325\textwidth}
    \vspace{5em}
    \hfill\includegraphics[width=0.3\textwidth]{ir_8.png}
    \captionof{figure}{Esempio di CFG}
  \end{wrapfigure}
  \noindent\par \begin{itemize}
    \item un BB \`e una seq.~di istruzioni in forma 3AC
      \begin{itemize}
        \item singolo \textit{entry point}: solo la prima istruzione puo essere raggiunta dall'esterno
        \item singolo \textit{exit point}: se eseguo la prima istr.~\textbf{devo eseguire tutte le altre} - garantisco che venga \textbf{eseguito interamente}
        \item Le chiamiamo sezioni single-entry, single-exit (possono essere sezioni anche piu grandi, ma le piu piccole di questo tipo sono i BB)
      \end{itemize}
    \item un arco connette due nodi $B_{i}\rightarrow B_{j}$ $\iff$ $b_{j}$ pu\`o eseguire dopo $B_{i}$ in qualche percorso del ctrl flow del programma
      \begin{itemize}
        \item prima istr.~di $B_{j}$ \`e target dell'istr.~di salto al termine di $B_{i}$
        \item[$\lor$] $B_{i}$ non ha un istr.~di salto come ultima istr.~(nodo \textit{falltrough}) e $B_{j}$ \`e suo unico successore
      \end{itemize}
    \item un CFG \textbf{normalizzato} ha i BB \textbf{massimali}
      \begin{itemize}
        \item non possono essere resi pi\`u grandi senza violare condizioni
        \item unisco i BB fallthrough che non hanno label all'inizio
        \item posso avere CFG non norm.~dopo qualche generico passo di ottimizzazione (non le facciamo accadere "spontaneamente")
      \end{itemize}
  \end{itemize}%
}


\paragraph{Algoritmo per la costruzione del CFG}~\\

\begin{enumerate}
  \item identificare il \textbf{leader} di ogni BB:
    \begin{itemize}
      \item la prima istruzione
      \item il target di un salto
      \item ogni istruzione dopo un salto
    \end{itemize}
  \item il BB \textbf{comincia} con il leader e \textbf{termina} con l'istruzione immediatamente precedente un nuovo leader (o l'ultima istruzione)
  \item \textbf{connettere} i BB tramite archi di 3 tipi:
    \begin{itemize}
      \item \textbf{fallthrough} (o fallthru): esiste solo un percorso che collega i due blocchi
      \item \textbf{true}: il secondo blocco \`e raggiungibile dal primo se un condizionale \`e \lstinline|true|
      \item \textbf{false}: il secondo blocco \`e raggiungibile dal primo se un condizionale \`e \lstinline|false|
    \end{itemize}
\end{enumerate}

\subsubsection{DG (Dependency Graph)}

I nodi di un DG sono istruzioni; un arco connette due nodi di cui \textbf{uno usa il valore definito dall'altro}. Sono indispensabili per l'\textit{instruction scheduling} e per mantenere il CPI della pipeline (ricordiamo quella RISC-V):\\

\noindent\begin{minipage}[c]{.6\textwidth}
  \begin{itemize}
    \item[$\curvearrowright$] IF Instruction Fetch
    \item[$\downarrow$] ID Instruction Decode
    \item[$\downarrow$] EXE Execute
    \item[$\downarrow$] MEM Memory Access
    \item[$\hookleftarrow$] WB Write Back
  \end{itemize}
\end{minipage}
\begin{minipage}[c]{.4\textwidth}
  \centering
  \includegraphics[width=.85\textwidth]{ir_9.png}
  \captionof{figure}{Esempio di data hazard}
\end{minipage}\\

\begin{example}[frametitle={Esempio: risultato di una \lstinline|add|}]
  Se in fase di decode provo a leggere il registro usato in una \lstinline|add| immediatamente precedente, questo ancora non contiene il risultato aggiornato (pronto appena tra 2 cicli) $\rightarrow$ \textbf{data hazard}, gestito solitamente dalla \textbf{forwarding unit} che bypassa MEM e WB e inoltra direttamente il dato 
\end{example}

\begin{emphasize}[frametitle={Soluzione generica (inefficiente)}]
  Per quanto i controlli vengano svolti dalla fw.~unit, in generale l'unico modo per evitare questo tipo di hazard \`e distanziare le istruzioni tra loro affinch\'e il dato sia disponibile $\rightarrow$ inserisco nop (cicli di stallo), ma vado a "rompere" l'IPC pari a a 1 della pipeline sempre piena ($<$ performance)
  \mdfsubtitle{Soluzione migliore}
  \textbf{scheduling} del programma, spostando istruzioni che non dipendono da quei registri al posto di aggiungere \lstinline|nop| $\rightarrow$ uno dei compiti principali di un backend, che per evitare di cercare le istruzioni libere "manualmente" sfrutta la IR di tipo DG che fornisce esattamente le informazioni sulle dipendenze tra istruzioni
\end{emphasize}

\subsubsection{DDG (Data Dependency Graph)}

\begin{itemize}
  \item specifico per multicore e parallelismo, usato per dare una rappresentazione tra le dipendenze dei \textbf{dati} - tipicamente i loop, a patto che non ci siano dipendenze di dato tra le varie iterazioni.
  \item tipicamente uso il \textbf{polyhedral model} $\rightarrow$ rappresento lo spazio delle it.~come un poliedro (a seconda del numero di loop innestati), che permette di capire se esiste qualche permutazione dei loop (direzione di attraversamento dello spazio delle iterazioni; ovvero ad esempio scambiare l'ordine dei loop) \textbf{non soggetta a dipendenze}
\end{itemize}

\subsubsection{CG (Call Graph)}

\noindent\begin{minipage}[c]{.6\textwidth}
  Rappresentazione gerarchica a grafo usata per ragionare sull'insieme delle potenziali chiamate tra funzioni della translation unit del sorgente

  \begin{emphasize}[frametitle={Nota}]
    Il compilatore ha visibilit\`a solo fino a livello dei singoli moduli: posso estendere le ottimizzazioni al massimo fino ai legami tra funzioni dello stesso modulo, quelle pi\`u ampie si spostano a framework di ott.~che agiscono es.~a livello di linker
  \end{emphasize}
\end{minipage}
\begin{minipage}[c]{.4\textwidth}
  \includegraphics[width=\textwidth]{ir_10.png}
\end{minipage}

\section{Ottimizzazione locale e Local Value Numbering}

\subsection{Scope dell'ottimizzazione}

\noindent\begin{minipage}[c]{.55\textwidth}
  Lo scope viene influenzato da come viene gestito il flusso di controllo in un programma

  \begin{itemize}
    \item ott.~\textbf{\textcolor{Emerald}{locale}}: entro un singolo BB, non si preoccupa del flusso
    \item ott.~\textbf{\textcolor{Orange}{globale}}: lavora a livello dell'intero CFG
    \item ott.~\textbf{\textcolor{Cerulean}{interprocedurale}}: lavora a livello del call graph, e quindi sui CFG di pi\`u funzioni
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.4\textwidth}
  \includegraphics[width=\textwidth]{lvn_1.png}
\end{minipage}

\subsection{Dead Code Elimination}

Cominciamo ragionando su ott.~locali come la DCE (sono \textit{dead code} le istr.~che definiscono una variabile mai utilizzata)

\noindent\begin{minipage}[b]{.65\textwidth}
  Sicuramente va tolta la def.~di c, ma poich\'e \lstinline|print| \textbf{non definisce nulla}, stando alla def.~\`e dead code! $\rightarrow$ Estendiamo la definizione dicendo che lo sono le istr \textbf{prive di side effects} che definiscono ... 
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
  \begin{lstlisting}
main {
  int a = 4;
  int b = 2;
  int c = 1;
  int d = a + b;
  print d;
}\end{lstlisting}
\end{minipage}

\subsubsection{Algoritmo per la DCE}

\begin{enumerate}
  \item $\forall$ istruzione in BB
    \begin{itemize}
      \item aggiungi operandi ad un metadato array \lstinline|used|
    \end{itemize}
  \item $\forall$ istruzione in BB
    \begin{itemize}
      \item se non ha destinazione e non ha side effects rimuovila
      \item altrimenti se la destinazione non corrisponde a nessuno degli elem di \lstinline|used| rimuovila
    \end{itemize} 
\end{enumerate}

\begin{lstlisting}
used = {};

for instr in BB:
  used += instr.args;

for instr in BB:
  if instr.dest &&
     instr.dest not in used:
delete instr\end{lstlisting}

A questo punto rendiamolo iterativo, per farlo eseguire fino a "convergenza" (elimino tutto il \textit{d-c})

\begin{lstlisting}
while prog changed:
{
  ... #alg
}\end{lstlisting}

\noindent\begin{minipage}[c]{.3\textwidth}
  \begin{lstlisting}
main {
  int a = 100;
  int a = 42;
  print a;
}\end{lstlisting}
\end{minipage}\hfill $\rightarrow$ \hfill
\begin{minipage}[c]{.62\textwidth}
  Consideriamo questo esempio in cui si ridefinisce una variabile: il nostro algoritmo corrente non elimina nulla perch\'e non gestisce le \textbf{dead stores}
\end{minipage}

\noindent $\rightarrow$ per estenderlo dobbiamo poter \textbf{rilevare gli assegnamenti multipli} di una variabile, e quindi anche \textbf{preoccuparci dell'ordine} delle istruzioni! (pi\`u complicato senza forma SSA)

\begin{itemize}
  \item dopo ogni istr.~teniamo traccia delle variabili definite, ma non usate
  \item se troviamo un altro assegnamento alla stessa variabile prima della fine del blocco sappiamo che quello precedente pu\`o essere eliminato
\end{itemize}

\begin{lstlisting}
last_def = {}; /* lista di variabili definite ma non usate (ptr alla piu' recente per le sole variabili mai usate) */

for instr in BB:
  last_def -= instr.args; /*rimuovo ogni argomento (operando) dell'istruzione corrente: se presente, e' un uso della variabile */

  if instr.dest in last_def:
    delete last_def[instr.dest]; /*se a questo punto la destinazione dell'istr. corrente e' presente in last_def posso sovrascrivere la definizione precedente*/
last_def[instr.dest]=instr;\end{lstlisting}

\begin{itemize}
  \item nota: per situazioni tipo \lstinline|x=2; x=x+3;| si vuole evitare che la seconda istr.~venga eliminata perch\'e non ho realizzato che \lstinline|x| \`e usata (non \`e \textit{d-c}) $\rightarrow$ per questo controllo prima gli usi e poi le definizioni
  \item anche questo algoritmo va ripetuto fino a convergenza
\end{itemize}


\subsection{Local Value numbering}

Tecnica utilizzata per considerare il concetto di ordine delle definizioni in assenza di propriet\`a di tipo SSA

osserviamo 3 pattern che forniscono opportunit\`a di eliminazione di codice ridondante:
\begin{itemize}
  \item dead code elimination: 1 variabile e piu valori
  \item copy propagation: 1 valore e piu variabili
  \item common subexpression elimination: 1 valore (in forma di espressione) e piu variabili
\end{itemize}

sono tutti modelli di computazione che si focalizzano sulle \textbf{variabili} $\rightarrow$ focalizzandoci sui \textbf{valori} possiamo eliminare tutte le forme di ridondanza

\begin{figure}[h]
  \centering
  \includegraphics[width=.65\textwidth]{lvn_2.png}
  \caption{Esempi di codice legati ai pattern citati}
\end{figure}

\begin{itemize}
  \item costruisco un metadato in forma di tabella che riscrive le espr.~(istruzioni) in funzione dei valori gia osservati $\rightarrow$ evitando di riassegnare lo stesso valore a piu variabili si evita la ridondanza
  \item analizzo le istruzioni in termini del value, e in caso questo coincida con entry della tabella gi\`a presenti punto direttamente a quella
\end{itemize}

\begin{example}
  \noindent\begin{minipage}[c]{.6\textwidth}
    \includegraphics[width=\textwidth]{lvn_3.png}
    \captionof{figure}{Esempio di ottimizzazione tramite LVN}
  \end{minipage}\hfill
  \begin{minipage}[c]{.3\textwidth}
    \centering
    \begin{lstlisting}
main {
  int a = 4;
  int b = 2;
  int sum1 = a + b;
  int prod = sum1 * sum1;
  print prod;
}\end{lstlisting}

  Risultato ottenuto
\end{minipage}
\end{example}


\noindent\begin{minipage}[c]{.3\textwidth}
  \begin{lstlisting}
#...
int sum1 = a + b;
int sum2 = b + a;\end{lstlisting}
\end{minipage}\hfill $\rightarrow$
\begin{minipage}[c]{.65\textwidth}
  Semplice variante del programma non riconosciuta dall'algoritmo (non conosce la pr.~commutativa della somma) $\rightarrow$ vado a \textbf{canonicalizzare} l'algoritmo, ovvero imporre un ordine numerico tra le entry (i valori) e usarle sempre in ordine crescente per le op.~commutative (di fatto svolto da tutti i compilatori)
\end{minipage}

\section{Data Flow Analysis}

\subsection{Cos'\`e la DFA}

La possiamo vedere come una \textbf{metodologia} o come un \textbf{framework} di analisi, applicabile a vari problemi di ottimizzazione (CP, CSE, DCE, ...)

\begin{itemize}
  \item analisi \textbf{locale}: si focalizza sull'effetto di ogni istruzione $\rightarrow$ posso comporre gli effetti di tutte le istr.~per derivare informazione dall'inizio del BB ad ogni istruzione
  \item analisi \textbf{globale} (DFA): simile, ma molto pi\`u complessa $\rightarrow$ analizza l'effetto di ogni BB, e poi ha una metodologia per comporre l'effetto dei BB ai \textbf{confini} degli stessi per derivare informazione
\end{itemize}

La DFA \`e \textbf{sensibile al flusso di controllo in una funzione} e prevede un'analisi \textbf{intraprocedurale} (singola funzione, singolo CFG)

\begin{example}
  \noindent\begin{minipage}[c]{.3\textwidth}
    \includegraphics[width=\textwidth]{dfa_1.png}
  \end{minipage}\hfill
  \begin{minipage}[c]{.65\textwidth}
    La DFA ci permette di derivare informazioni da un CFG, ad esempio $\forall$ variabile \lstinline|x| in ogni punto del grafo:
    \begin{itemize}
      \item qual \`e il suo valore?
      \item quale "definizione" la definisce?
      \item la definizione \`e ancora "valida" (\textit{live})?
    \end{itemize}
  \end{minipage}

  \begin{emphasize}
    Queste risposte normalmente le otteniamo grazie alla forma SSA delle istruzioni e alla struttura di LLVM, ma qui stiamo ancora "costruendo" la forma SSA
  \end{emphasize}
\end{example}

\subsubsection{Rappresentazione del programma statica o dinamica}

\begin{itemize}
  \item \textbf{statica}: programma finito, un pezzo di codice $\rightarrow$ molto facile da analizzare
  \item \textbf{dinamica}: pu\`o avere infiniti percorsi di esecuzione, rappresenta una possibile esecuzione reale!
    \begin{itemize}
      \item es. loop che si basa sull'analisi di un input
    \end{itemize}
    \begin{emphasize}
      condizioni che un compilatore \textbf{non pu\`o analizzare}, soprattutto non in maniera statica e \textit{finita} in termini di possibili istanze $\rightarrow$ spesso si passano al comp.~informazioni di "profiling" misurate durante ripetute esecuzioni del programma pre-ottimizzato
    \end{emphasize}
\end{itemize}

\noindent $\rightarrow$ Con la DFA siamo in grado di dire qualcosa \textbf{per ogni punto del programma}, combinando informazioni relative a \textbf{tutte le possibili istanze} dello stesso p.to

\subsubsection{Effetti di istruzioni e BB}

Effetti di un'istruzione: (\lstinline|a = b + c;|)
\begin{itemize}
  \item \textbf{uses}: delle variabili (\lstinline|b,c|)
  \item \textbf{kills}: una precedente definizione (\lstinline|a|)
  \item \textbf{defines}: una variabile (\lstinline|a|)
\end{itemize}
Combinando gli effetti delle singole istr.~si definiscono gli \textbf{effetti di un BB}:
\begin{itemize}
  \item \textbf{uso localmente esposto} (\textit{locally exposed use}): in un BB \`e un uso di una var.~che non \`e preceduto nel BB da una definizione della stessa variabile
  \item ogni definizione di una var.~nel BB \textbf{killa} tutte le definizioni delal stessa variabile che \textbf{raggiungono} il BB
  \item \textbf{definizione localmente disponibile} (\textit{locally available definition}): ultima definizione di una variabile nel BB
\end{itemize}

\newpage
\begin{example}
  \noindent \begin{minipage}[c]{.45\textwidth}
    \begin{lstlisting}[numbers=right,numbersep=-10pt]
t1 = r1+r2
r2 = t1
t2 = r2+r1
r1 = t2
t3 = r1*r1
r2 = t3
if r2>100 goto L1\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}[c]{.55\textwidth}
  \begin{itemize}
    \item usi loc.~esposti: 1 (\lstinline|r1,r2|), 3 (\lstinline|r2|), ...
    \item kill: 2 (\lstinline|r2|), ...
    \item definizioni loc.~disponibili: 6 (\lstinline|r2|), 5 (\lstinline|t3|), ...
  \end{itemize}
\end{minipage}
\end{example}

\subsection{Reaching Definitions}

\noindent \begin{minipage}[c]{.6\textwidth}
  Primo esempio di problema inquadrabile e risolvibile mediante DFA
  \begin{itemize}
    \item ogni istruzione di assegnamento \`e una definizione
    \item una definizione \textit{d} raggiunge (reaches) un punto it\textit{p} se esiste un percorso da \textit{d} a \textit{p} tale per chu d non \`e uccisa (sovrascritta) lungo quel percorso
  \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{dfa_2.png}
  \captionof{figure}{Esempio}
\end{minipage}

Definizione del problema:
\begin{itemize}
  \item determinare \textbf{per ogni punto} del programma se \textbf{ogni definizione} nel programma \textbf{raggiunge} quel punto
  \item[$\rightarrow$] usiamo un \textbf{bit vector} per ogni istruzione, con lunghezza del vettore pari al numero di definizioni $\Rightarrow$ diventa una sorta di matrice con righe le istruzioni, colonne le definizioni
\end{itemize}

\begin{example}
  \centering
  \includegraphics[width=.45\textwidth]{dfa_3.png}\hfill
  \includegraphics[width=.35\textwidth]{dfa_4.png}\\
  \includegraphics[width=.45\textwidth]{dfa_5.png}\hfill
  \includegraphics[width=.45\textwidth]{dfa_6.png}
\end{example}


\subsubsection{Schema DFA}

\noindent\begin{minipage}[c]{.67\textwidth}
  Consideriamo un flow graph:
  \begin{itemize}
    \item aggiungiamo un \textbf{BB entry} e uno \textbf{exit} $\rightarrow$ garantisco \textit{single-entry} e \textit{single-exit} points
    \item per stabilire l'effetto del codice \textbf{in ciascun BB} uso delle \textbf{funzioni di trasferimento} $f_{B}$, che correlano $out[B], in[B]$ per un dato BB $B$
    \item stabilisco l'effetto \textbf{del flusso di controllo} in base alla vicinanza dei blocchi: correlo $out[B_{i}], in[B_{j}]$ di blocchi adiacenti
    \item infine dobbiamo solo risolvere le equazioni 
  \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{dfa_7.png}
\end{minipage}

\vspace{-1em}
\begin{emphasize}
  Nota: stabiliamo anche una cosiddetta \textbf{boundary condition}, ovvero qual \`e l'informazione che riceve il primo BB dall'\textit{entry block} $\rightarrow$ per le Reaching Definitions stabiliamo $out[entry] = \emptyset$
\end{emphasize}


\subsubsection{Effetti di uno statement}

\begin{itemize}
  \item la fz.~di trasferimento di uno statement astrae l'esecuzione rispetto al problema di interesse
  \item per uno statement $s$ (\lstinline|d: x = y + z|):
    \begin{itemize}
      \item $Gen[s]$: definizioni \textbf{generate} ($Gen[s] = \lbrace\texttt{d}\rbrace$)
      \item definizioni \textbf{propagate}: $in[s] - Kill[s]$ dove $Kill[s]$ sono le altre definizioni di \lstinline|x| nel resto del programma
      \item funzione di trasferimento di uno statement $s$:
        \begin{equation*}
          out[s] = f_{s}(in[s]) = Gen[s] \cup (in[s] - Kill[s]) 
        \end{equation*}

    \end{itemize}

  \item in altre parole, $Gen$ \`e l'insieme di propriet\`a \textbf{generate} dal blocco, $Kill$ l'insieme di quelle \textbf{eliminate} nel blocco e $in$ l'insieme di quelle \textbf{ereditate}
    \begin{emphasize}
      Stiamo lavorando su bit vector $\rightarrow$ una $f_{s}$ riempie $out[s]$ a partire da $in[s]$ e applicando qualche tipo di calcolo
    \end{emphasize}
\end{itemize}

\vspace{-2em}
\subsubsection{Effetti di un BB}

\begin{itemize}
  \item la fz.~di trasferimento di un BB compone linearmente le fz.~dei suoi \textit{statements}
    \begin{equation*}
      \begin{split}
        \boxed{out[B]} = f_{B}(in[B]) &= f_{dn}\cdot\ldots\cdot f_{d0} \\
                                      &= \boxed{Gen[B] \cup (in[B] - Kill[B])}
      \end{split}
    \end{equation*}
    \begin{itemize}
      \item $Gen[B]$: definizioni localmente disponibili (\textbf{a fine BB})
      \item $Kill[B]$: definizioni uccise da B \textbf{in tutto il programma}
    \end{itemize}
  \item una $f_{B}$ associa \textbf{incoming reaching definitions} $\rightarrow$ \textbf{outgoing reaching definitions}
\end{itemize}

\vspace{-1em}
\begin{example}
  \begin{center}
    \includegraphics[width=.35\textwidth]{dfa_2.png}\hfill
    \includegraphics[width=.55\textwidth]{dfa_8.png}
  \end{center}
  \begin{emphasize}
    NB: in $Kill[B1]$ \lstinline|d0| e \lstinline|d2| si uccidono a vicenda $\rightarrow$ l'approccio bit-vector \textbf{non ha nozione di ordine di esecuzione}
  \end{emphasize}
\end{example}

\vspace{-1em}
\subsubsection{Effetti degli archi aciclici}

In caso di predecessori multipli, devo decidere con che criterio unire l'informazione:
\begin{itemize}
  \item nodo di unione (\textbf{join}): nodo con \textbf{predecessori} multipli
  \item operatore di \textbf{meet} ($\wedge$): $in[B] = out[p_1] \cup \ldots \cup out[p_n]$ con $p_1,\ldots,p_n$ tutti predecessori di $B$
    \begin{emphasize}
      Il meet operator \`e \textbf{specifico del problema}, non dell'analisi mediante DFA!
      \noindent $\rightarrow$ Per il problema delle reaching definitions, usiamo $\wedge = \cup$ in quanto la condizione deve essere verificata per \textit{almeno un percorso}
    \end{emphasize}
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=.65\textwidth]{dfa_9.png}
  \caption{Esempio (continua)}
\end{figure}

\subsubsection{Effetti degli archi ciclici e condizioni iniziali}

Gli archi ciclici (\textit{backedges}) possono cambiare le loro $out$ o non averle ancora calcolate durante l'esecuzione dell'algoritmo:
\begin{itemize}
  \item \textbf{itero fino a convergenza}
  \item definisco delle \textbf{condizioni iniziali} per inizializzare ogni BB (similmente alla boundary condition): stabilisco $out[B] = \emptyset$ (specifico per Reaching Defs.)
\end{itemize}

\subsubsection{Algoritmo iterativo}

\begin{lstlisting}
input: control flow graph CFG = (N, E, Entry, Exit)
// Boundary condition
  out[Entry] = $\emptyset$

// Initialization for iterative algorithm
  for each basic block B other than Entry
    out[B] = $\emptyset$

// Iterate
  while (changes to any out[] occur) {
    in[B] = $\cup$ (out[p]), for all predecessors p of B
    out[B] = f$_B$(in[B])   // $\textcolor{codeblue}{out[B] = Gen[B] \cup (in[B] - Kill[B])}$
}\end{lstlisting}

\begin{example}[frametitle={Esempio di eseguzione}]
  \noindent\begin{minipage}[c]{.45\textwidth}
    \begin{emphasize-blue}
      La rappresentazione del bit-vector segue l'ordine di numerazione delle definizioni: \lstinline|IN[B1] = <000 00 0 0>| con \lstinline|d1| primo bit, ...
    \end{emphasize-blue}

    Nota: dopo la seconda iterazione $out[B2]$ non cambia pi\`u
  \end{minipage}
  \begin{minipage}[c]{.55\textwidth}
    \includegraphics[width=\textwidth]{dfa_10.png}
  \end{minipage}
\end{example}

\subsubsection{Algoritmo Worklist}

La worklist contiene le variabili che devono ancora essere processate. Quando la worklist \`e vuota l'algoritmo termina.

\begin{lstlisting}
input: control flow graph CFG = (N, E, Entry, Exit)

// Initialize
  out[Entry] = $\emptyset$  // can set out[Entry] to special def
                  // if reaching then undefined use
  for all nodes i
    out[i] = $\emptyset$    // can optimize by out[i] = gen[i]
  ChangedNodes = N

// Iterate
  while ChangedNodes $\neq \emptyset$ {
  remove i from ChangedNodes
  in[i] = $\cup$ (out[p]), for all predecessors p of i
  oldout = out[i]
  out[i] = f$_i$(in[i])     // $\textcolor{codeblue}{out[B] = Gen[B] \cup (in[B] - Kill[B])}$
  if (oldout $\neq$ out[i]){
    for all successors s of i
      add s to ChangedNodes
  }
}\end{lstlisting}

\subsection{Liveness Analysis}

\subsubsection{Live Variable Analysis}

\noindent\begin{minipage}[c]{.7\textwidth}
  \paragraph{Definizione:}

  Una variabile \lstinline|v| \`e \textbf{viva} (\textit{live}) in un punto \textit{p} del programma se il valore di \lstinline|v| \`e usato lungo qualche percorso del FG a partire da \textit{p} (altrimenti \`e \textbf{morta})\\

  \paragraph{Motivi dell'analisi:}

  (oltre alla DCE) es. \textit{register allocation}: i registri reali sono limitati - la \textit{ra} \`e l'operazione che cerca di evitare le spill in cache e in memoria, cercando i casi in cui risulta possibile \textbf{riutilizzare} un certo registro $\rightarrow$ dipende evidentemente dalla liveness di una variabile\\

  \paragraph{Definizione del problema DFA}

  \begin{itemize}
    \item devo stabilire $\forall$ BB quali sono le variabili \textbf{vive} in ciascuno di essi
    \item bit-vector di lunghezza pari al numero di variabili
  \end{itemize}
\end{minipage}
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{dfa_11.png}
  \includegraphics[width=\textwidth]{dfa_12.png}
\end{minipage}

\subsubsection{Forward e Backward analysis}

es.~\textit{Reaching Definitions}: che informazione sto cercando per capire se una definizione raggiunge o meno un punto \textit{p} del programma? Devo analizzare il "\textbf{passato}" - gli statement tra la definizione e \textit{p} per cercare eventuali kill (analizzo da \textit{entry} a \textit{p})

\noindent $\rightarrow$ Nel contesto della \textit{Liveness Analysis} invece, devo analizzare il "\textbf{futuro}" - cerco gli usi della variabile da \textit{exit} a \textit{p}

\subsubsection{Funzione di trasferimento}

Per la formulazione pi\`u tipica della \textit{LA} (ce ne sono diverse, a seconda delle fonti) usiamo
\begin{itemize}
  \item l'insieme delle variabili vive che pu\`o \textbf{generare} un BB ($Use[B]$)
  \item l'insieme delle variabili \textbf{definite} nel BB ($Def[B]$)
  \item le variabili vive in ingresso che il bb pu\`o propagare ($Out[B] - Def[B]$)
  \item[$\rightarrow$] \textbf{funzione di trasferimento} per il blocco $B$:
    \begin{equation*}
      In[B] = Use[B] \cup (Out[B] Def[B]) 
    \end{equation*}
\end{itemize}

\paragraph{Flow Graph}

\begin{itemize}
  \item $In[B] = f(Out[B])$ (\textit{backward analysis}, contrario rispetto a \textit{Reaching Definitions})
  \item \textbf{join} node: nodo con \textbf{successori} multipli
  \item \textbf{meet} operator ($\wedge$): $out[B] = in[s_1] \cup \ldots \cup in[s_n]$ con $s_1,\ldots,s_n$ tutti predecessori di $B$\\
    (ricorda: $\cup \implies \exists$ almeno un percorso)
\end{itemize}

\subsubsection{Algoritmo iterativo}

\noindent\begin{minipage}[c]{.3\textwidth}
  \begin{itemize}
    \item \textbf{boundary condition}: $\emptyset$
    \item \textbf{starting conditions}: $\emptyset$
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.67\textwidth}
  \begin{emphasize}
    La scelta delle condizioni iniziali deriva principalmente dalla scelta del meet operator - se per qualsiasi motivo scegliessimo come meet operator $\cap$, usare $\emptyset$ impedirebbe qualsiasi propagazione dell'informazione
  \end{emphasize}
\end{minipage}

\vspace{1em}
\begin{lstlisting}
input: control flow graph CFG = (N, E, Entry, Exit)

// Boundary condition
  in[Exit] = $\emptyset$

// Initialization for iterative algorithm
  for each basic block B other than Exit
    in[B] = $\emptyset$

// Iterate
  while (changes to any in[] occur) {
    for each basic block B other than Exit {
      out[B] = $\cup$ (in[s]), for all successors s of B
      in[B] = f$_B$(out[B])   // $\textcolor{codeblue}{in[B] = Use[B] \cup (Out[B] - Def[B])}$
    }
}\end{lstlisting}

\begin{emphasize}
  A convergenza si arriva \textbf{indipendentemente dall'ordine} in cui calcolo le funzioni dei BB (cambia al massimo il numero di iterazioni per arrivarci)
\end{emphasize}

\begin{example}
  \begin{center}
    \includegraphics[width=.5\textwidth]{dfa_13.png}
  \end{center}
\end{example}

\subsection{Framework per DFA}

Possiamo usare una tabella di questo tipo per descrivere problemi nell'ambito della DFA:

\begin{center}
  \includegraphics[width=.65\textwidth]{dfa_14.png}
\end{center}

\subsection{Available Expressions}

Altro problema modellabile tramite DFA, utile ad es.~per la Common Subexpression Elimination

\begin{center}
  \begin{minipage}[c]{.4\textwidth}
    \begin{lstlisting}[linewidth=.6\linewidth]
if (...) {
  x = m + n;
} else {
  y = m + n;
}
z = m + n;\end{lstlisting}
  \lstinline|m + n| \`e ridondante perch\'e gi\`a calcolato
\end{minipage}\quad
\begin{minipage}[c]{.5\textwidth}
    \begin{lstlisting}[linewidth=.48\linewidth]
if (...) {
  x = m + n;
} else {
  ...
}
z = m + n;\end{lstlisting}
Cosa cambia se non viene calcolato nel ramo \lstinline|else|?
\end{minipage}
\end{center}

Il concetto di available expr.~\`e necessario come maniera rigorosa di ragionare sulla \textbf{ridondanza}

\paragraph{Definizione del problema:}

\begin{itemize}
  \item \textbf{dominio}: tutte le espressioni del programma
    \begin{itemize}
      \item consideriamo solo espr.~binarie del tipo $x \oplus y$
    \end{itemize}
  \item un'espressione $x \oplus y$ \`e \textbf{available} in un punto \textit{p} del programma se \textbf{ogni} percorso che parte da \textit{Entry} e arriva a \textit{p} valuta l'espressione
  \item \textbf{funzione di trasferimento} di un BB:
    \begin{equation*}
      f_{B}(x) = Gen[B] \cup (x - Kill[B])
    \end{equation*}
    \begin{itemize}
      \item un blocco \textbf{genera} l'espressione $x \oplus y$ se la valuta e \textbf{non ridefinisce in seguito} \textit{x} o \textit{y}
      \item un blocco \textbf{uccide} l'espr. $x \oplus y$ se assegna (o potrebbe assegnare) un valore a \textit{x} o \textit{y} e non ricalcola successivamente l'espressione
    \end{itemize}
  \item \textbf{direzione di analisi}: \textbf{forward}
    \begin{itemize}
      \item nell'analisi delle \textit{Available Expressions} eliminiamo un'espr.~perch\'e \`e stata calcolata \textbf{in passato}
      \item (nell'analisi delle \textit{Live Variables} eliminiamo una var.~perch\'e non verr\`a usata \textbf{in futuro})
    \end{itemize}
  \item equazioni $Out$: $Out[B] = f_{b}(In[B])$
  \item equazioni $In$: $In[B] = \wedge _{p\in pred(B)}(Out[B])$
  \item \textbf{meet} operator ($\wedge$): $\cap$ ("\textbf{ogni percorso} che parte da Entry [...]")
    \begin{center}
      \includegraphics[width=.5\textwidth]{dfa_15.png}
    \end{center}
\end{itemize}

\noindent\begin{minipage}[c]{.5\textwidth}
  \begin{itemize}
    \item \textbf{boundary conditions}: $Out[Entry] = \emptyset$
    \item \textbf{initial conditions}:
      \begin{itemize}
        \item $\textcolor{red}{\xcancel{\textcolor{black}{Out[B_{i}] = \emptyset}}}$ $\rightarrow$ il meet op.~\`e $\cap$
        \item $Out[B_{i}] = \mathcal{U}$
      \end{itemize}
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.4\textwidth}
  \includegraphics[width=\textwidth]{dfa_17.png}
  \captionof{figure}{Tabella completa}
\end{minipage}

\begin{example}
  \noindent\begin{minipage}[c]{.65\textwidth}
    Risolto su fogli, dominio $\lbrace$\lstinline|a=b,a*b,a-1|$\rbrace$
    \begin{emphasize}
      Essendo la DFA per natura \textbf{statica}, considera sempre il caso \textbf{peggiore} di esecuzione.\\
      Il risultato dell'analisi sarebbe diverso se conoscessi l'istanza specifica - sapendo ad es.~che BB4 sar\`a falso potrei fare delle assunzioni che, staticamente, non posso fare $\rightarrow$ ritorno conservativamente al caso peggiore che il loop esegua almeno una volta
    \end{emphasize}
  \end{minipage}\hfill
  \begin{minipage}[c]{.3\textwidth}
    \includegraphics[width=\textwidth]{dfa_16.png}
  \end{minipage}
\end{example}

\subsection{Very Busy Expressions (Assignment 2)}

\noindent\begin{minipage}[c]{.6\textwidth}
\begin{itemize}
  \item Un'espressione \`e \textbf{very busy} in un punto \textit{p} se, indipendentemente dal percorso preso da \textit{p}, l'espressione viene usata prima che uno dei suoi operandi venga definito.
  \item Un'espressione \lstinline|a+b| \`e \textbf{very busy} in un punto \textit{p} se \lstinline|a+b| \`e valutata in tutti i percorsi da \textit{p} a \textit{Exit} e non c'\`e una definizione di \lstinline|a| o \lstinline|b| lungo tali percorsi
\end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.35\textwidth}
\includegraphics[width=\textwidth]{dfa_18.png}
\end{minipage}
\\

Dove
\begin{itemize}
  \item $Gen[b]$: espressioni valutate all'interno del Basic Block
  \item $Kill[b]$: un'espressione viene uccisa quando un suo operando viene ridefinito all'interno di \textit{b}
\end{itemize}

\subsection{Dominator Analysis (Assignment 2)}

\noindent \begin{minipage}[c]{.6\textwidth}
\begin{itemize}
  \item In un CFG diciamo che un nodo \textit{X} \textbf{domina} un altro nodo \textit{Y} se il nodo \textit{X} appare in ogni percorso del grafo che porta dal blocco \textit{Entry} al blocco \textit{Y}
  \item annotiamo ogni basic block $B_{i}$ con un insieme $Dom[B_{i}]$:
  \begin{itemize}
    \item $B_{i}\in Dom[B_{j}] \iff B_{i}$ domina $B_{j}$
    \item $B_{i}\in Dom[B_{i}]$: per definizione un nodo domina se stesso
  \end{itemize}
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.35\textwidth}
\includegraphics[width=\textwidth]{dfa_19.png}
\end{minipage}

\subsection{Constant Propagation (Assignment 2)}

\begin{itemize}
  \item L'obiettivo della \textit{constant propagation} \`e quello di determinare in quali punti del programma le variabili hanno un valore costante.
\end{itemize}
\noindent \begin{minipage}[c]{.6\textwidth}
\begin{itemize}
  \item L'informazione da calcolare per ogni nodo del CFG \`e un insieme di coppie del tipo\\\lstinline|<variabile,valore costante>|.
  \item Se abbiamo la coppia \lstinline|<x,c>| al nodo \textit{n}, significa che \lstinline|x| \`e garantito avere il valore \lstinline|c| ogni volta che \textit{n} viene raggiunto durante l'esecuzione del programma.
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.35\textwidth}
\includegraphics[width=\textwidth]{dfa_20.png}
\end{minipage}
\begin{itemize}
  \item $Gen[b]$ rappresenta l'insieme di nuovi assegnamenti (all'interno di \textit{b}) con valore costante. I casi di interesse sono:
    \begin{itemize}
      \item \lstinline|x = c| con \lstinline|c| costante: $Gen[b] = Gen[b] \cup (x,c)$
      \item \lstinline|x = c1|$\oplus$\lstinline|c2| con \lstinline|c1,c2| costanti: $Gen[b] = Gen[b] \cup (x,c1\oplus c2)$ (calcolando il valore dell'espressione)
      \item \lstinline|x = c|$\oplus$\lstinline|y| o \lstinline|x = y|$\oplus$\lstinline|c| con \lstinline|c| costante:
        \begin{enumerate}
          \item controlliamo che y sia presente nell'insieme in input: $\exists (y,v_{y}) \in In[b]$
          \item se \`e vero, $Gen[b] = Gen[b] \cup (x,e)$ con $e = c\oplus v_{y}$ (o $v_{y}\oplus c$)
        \end{enumerate}
      \item \lstinline|x = y|$\oplus$\lstinline|z|, valutando \lstinline|y| e \lstinline|z| allo stesso modo del caso precedente:
        \begin{equation*}
          \exists (y,v_{y}), (z,v_{z})\in In[b] \implies Gen[b] = Gen[b] \cup (x,e) \text{~con~} e=v_{y}\oplus v_{z}
        \end{equation*}

        \begin{mdframed}
          Nel caso di definizioni multiple di \lstinline|x|, riteniamo valida solamente l'ultima all'interno di \textit{b}
        \end{mdframed}
    \end{itemize}
  \item $Kill[b]$: ogni definizione \lstinline|x = expr| uccide le coppie $(x,c)\in \mathcal{D}$ (dominio)
\end{itemize}

\section{Loops e UD-DU chains}

\subsection{Cos'\`e un loop}

Sapendo che un programma spende la maggior parte del tempo nei loop, l'obiettivo \`e definire un loop \textbf{in termini di teoria dei grafi} (CFG), ovvero \textbf{indipendentemente dalla loro sintassi e dal tipo} (\lstinline|for|, \lstinline|while|, \lstinline|goto|, costrutti stile assembly con salti condizionati e non...)\\

\noindent\begin{minipage}[c]{.45\textwidth}
  \textbf{Elementi chiave per riconoscere un loop}:
  \begin{itemize}
    \item gli archi devono formare almeno un ciclo
    \item (fondamentale) \textbf{singolo entry point} (tutti gli archi, se multipli, devono entrare nello stesso punto)
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.5\textwidth}
  \begin{example}[frametitle={}]
    \noindent\begin{minipage}[c]{.7\textwidth}
      Generalmente, non tutti i \textit{cicli} sono un "loop" da un p.to di vista dell'ottimizzazione: $\textcolor{red}{c\rightarrow d}$ \`e un loop, $\textcolor{blue}{a\rightarrow c \rightarrow d}$ no
    \end{minipage}\hfill
    \begin{minipage}[c]{.25\textwidth}
      \includegraphics[width=\textwidth]{uddu_1.png}
    \end{minipage}
  \end{example}
\end{minipage}

\subsubsection{Definizioni formali}

\paragraph{Dominator}

Un nodo \textit{d} domina un nodo \textit{n} in un grafo ( \textit{d dom n} ) se ogni percorso da \textit{ENTRY} a \textit{n} passa per \textit{d}
\paragraph{Immediate dominator}

L'\textbf{ultimo dominator} di \textit{n} su qualsiasi percorso da \textit{ENTRY} a \textit{n}

\textit{m} domina \textbf{immediatamente} (strettamente) \textit{n} (\textit{m sdom n}) $\iff$ $m\, dom \,n\land m \neq n$

\paragraph{Dominator Tree}

Modo per rappresentare la propriet\`a di dominanza \textbf{in forma di albero}

\begin{itemize}
  \item $a \rightarrow b$ nel dominator tree $\iff$ \textit{a sdom b}
  \item \textbf{non compaiono} le relazioni di \textbf{"auto-dominazione"} (ogni nodo domina sempre se stesso, ma nell'albero non serve rappresentarlo)
  \newpage
  \item \textit{ENTRY} \`e la \textbf{radice}
  \item ogni nodo \textit{d} domina solo \textbf{i suoi discendenti} nell'albero
\end{itemize}

\begin{example}[frametitle={Esempi di DT}]
  \begin{minipage}[c]{.24\textwidth}
    \includegraphics[width=\textwidth]{uddu_2.png}
  \end{minipage}
  \begin{minipage}[c]{.24\textwidth}
    \includegraphics[width=\textwidth]{uddu_3.png}
  \end{minipage}\hfill\vline\hfill
  \begin{minipage}[c]{.48\textwidth}
    \includegraphics[width=\textwidth]{uddu_4.png}
    \label{example-dt}
  \end{minipage}
\end{example}


\subsubsection{Loop naturali}

Nonostante le diverse forme trovate nei sorgenti, dal punto di vista dell'analisi ci interessa solo che abbiano propriet\`a che facilitino l'ottimizzazione, ovvero
\begin{itemize}
  \item \textbf{singolo entry point}: header $\rightarrow$ l'header \textbf{domina tutti i nodi nel loop}
  \item back edge, ossia arco la cui testa domina la propria coda (tail $\rightarrow$ head): un back edge \textbf{deve far parte di almeno un loop}
\end{itemize}

\subsection{Identificare i loop naturali}

\begin{enumerate}
  \item trovare le relazioni di \textbf{dominanza}
  \item identificare i \textbf{back edges}
  \item trovare il \textbf{loop naturale associato} al back edge
\end{enumerate}

\subsubsection{Trovare i dominatori}

Lo impostiamo in termini di DFA (vedi secondo assignment)

\subsubsection{Trovare i back edges}

\noindent \begin{minipage}[c]{.67\textwidth}
  Usiamo un algoritmo basato su \textit{depth-first traversal
  }
  \begin{itemize}
    \item inizio alla radice e visito \textbf{ricorsivamente} i figli di ogni nodo \textbf{in qualsiasi ordine}
    \item importante la "velocit\`a di discesa": prima scendo ed esploro \textbf{in profondit\`a}, a prescindere dall'ordine
  \end{itemize}

  Il percorso della visita definisce un \textbf{depth-first spanning tree (DFST)}:
  \begin{itemize}
    \item archi \textbf{solidi}: struttura del DT
    \item archi \textbf{tratteggiati}: altri archi del CFG
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{uddu_5.png}
\end{minipage}

\paragraph{Categorizzazione degli archi}

\begin{itemize}
  \item \textbf{advancing} (A) edges: da antenato a discendente, ovvero gli archi detti \textit{proper} - gli archi solidi sono tutti A
  \item \textbf{retreating} (R) e.: da discendente a antenato (non necessariamente proper $\rightarrow$ da un nodo a se stesso) - solo archi tratteggiati
  \item \textbf{cross} (C) e.: archi tali per cui nessuno dei due nodi \`e antenato dell'altro
\end{itemize}

\begin{emphasize}
  Se disegniamo il DFST in modo che i figli siano aggiunti da sx a dx nell'ordine di visita, allora i cross edges vanno sempre da dx a sx
\end{emphasize}

\noindent \begin{minipage}[c]{.67\textwidth}
  \paragraph{Algoritmo}

  \begin{itemize}
    \item esegui una depth-first search
    \item $\forall$ retreating edge $t \rightarrow h$ controlla se \textit{h dom t}
  \end{itemize}


  \begin{emphasize}
    Vado a riconoscere i \textit{back edges} come casi specifici di \textit{retreating edges} $\rightarrow$ la maggior parte dei programmi hanno control flow \textbf{riducibili}, ovvero tutti i \textit{retreating edges} sono anche back
  \end{emphasize}
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{uddu_5b.png}
\end{minipage}

\subsubsection{Trovare il loop naturale associato}

Il loop naturale di un back edge \`e il \textbf{pi\`u piccolo} insieme di nodi che \textbf{include head e tail} del back edge e \textbf{non ha predecessori fuori da questo insieme} 

\paragraph{Algoritmo}
\begin{itemize}
  \item eliminare \textit{h} dal CFG
  \item trovare i nodi che raggiungono \textit{t}: questi (pi\`u \textit{h}) formano il \textbf{loop naturale} $t \rightarrow h$
\end{itemize}


$\rightarrow$ genericamente rappresento i loop in maniera "header-centrica"

\subsection{Preheader}

\noindent\begin{minipage}[c]{.65\textwidth}
  Spesso (propedeutico a varie ott.) \`e necessario garantire che, quando arrivo all'header, ci arrivo tramite arco \textit{fallthrough}: es.~LICM (caso di code hoisting), che sposta un'istr.~in un punto comune al control flow di tutte le iterazioni del loop\\
  $\rightarrow$ tipicamente inserisco un blocco \textit{preheader} appena prima del loop, apposta per inserire le istr.~"hoistate"
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{uddu_6.png}
\end{minipage}

\begin{emphasize}
  Per manipolare i loop in LLVM esistono primitive per recuperare proprio questi blocchi fondamentali (poi per navigare nel resto dei BB usiamo gli iteratori, sia seguendo l'ordine del CFG sia non)
\end{emphasize}

\subsection{Use-def e Def-use chains}

La capacit\`a di ricollegare in maniera efficiente la definizione di una variabile a tutti i suoi usi (per esempio per propagare un risultato) \`e indispensabile all'ottimizzazione $\rightarrow$ per questo vengono previsti questi riferimenti nell'IR LLVM

\subsubsection{Dove viene definita o usata una variabile}

Le definizioni di DU e UD chain \textbf{precedono quella di SSA} $\rightarrow$ lo scope lessicale del programma in cui si potevano trovare gli usi della variabile era molto pi\`u ampio

\begin{example}[frametitle={Esempi}]
  \noindent 
  \begin{minipage}[c]{.65\textwidth}
    \begin{itemize}
      \item LICM: se le var.~usate nell'espressione che definisce \lstinline|A| sono \textbf{definite all'interno del loop}, non posso spostare la definizione fuori
      \item CP: per un dato uso di \lstinline|X|, trovo le sue \textit{reaching definitions} (es.~\lstinline|X = Y|) e, se non sono ridefinite (appunto "reaching") vado a sostituirne gli usi
    \end{itemize}

  \end{minipage}\hfill
  \begin{minipage}[c]{.25\textwidth}
    \includegraphics[width=\textwidth]{uddu_7.png}
  \end{minipage}

  \noindent A questo tipo di ottimizzazioni beneficia molto poter scorrere agevolmente le relazioni DU delle variabili $\rightarrow$ vogliamo un'IR che lo preveda e dunque permetta una \textbf{forma di analisi "sparsa"} (ignoro tutte le istruzioni non relative alla var.~correntemente analizzata)
\end{example}

In generale le catene possono essere onerose (per $N$ defs e $M$ uses, $O(NM)$ spazio e tempo) $\rightarrow$ Possiamo semplificare ulteriormente se riusciamo a \textbf{ridefinire completamente anche il nome della variabile ad ogni sua ridefinizione} (ci avviciniamo a SSA) (permette catene sensibilmente pi\`u corte)

\noindent\begin{minipage}[b]{.45\textwidth}
  \centering
  \includegraphics[width=.65\textwidth]{uddu_8.png}
  \captionof{figure}{Un altro motivo per rinominare le ridefinizioni \`e che occorrenze della stessa variabile potrebbero essere scorrelate e dunque ottimizzabili separatamente}
\end{minipage}\hfill
\begin{minipage}[b]{.5\textwidth}
  \centering
  \includegraphics[width=.65\textwidth]{uddu_9.png}
  \captionof{figure}{Esempio di catena onerosa. Ulteriore soluzione \`e \underline{limitare ogni variabile a una definizione}}
\end{minipage}

\section{Static Single Assignment (SSA)}

\noindent\begin{minipage}[t]{.4\textwidth}
  \vspace{.5em}
  Forma di IR dove ad ogni variabile viene \textbf{assegnato un valore solo una volta}\\ $\rightarrow$ Facile dentro ad un BB, ma cosa succede nei punti di \textbf{join} di un CFG?\\
  $\rightarrow$ \textbf{usiamo una notazione fittizia: la $\Phi$ function}
\end{minipage}\hfill
\begin{minipage}[t]{.58\textwidth}
  \begin{example}[frametitle={Local value numbering}]
    \begin{center}
      \includegraphics[width=\textwidth]{uddu_10.png}
    \end{center}

    All'interno di un BB posso fare \textit{local value numbering}, visitando ogni istruzione in ordine:
    \begin{itemize}
      \item LHS: assegna ad una nuova versione della variabile
      \item RHS: usa la versione pi\`u recente della stessa
    \end{itemize}

  \end{example}
\end{minipage}

\noindent\begin{minipage}[c]{.4\textwidth}
  \begin{lstlisting}
c $\leftarrow$ 12
if (i) {
  a $\leftarrow$ x + y
  b $\leftarrow$ a + x
} else {
  a $\leftarrow$ b + 2
  c $\leftarrow$ y + 1
}
a $\leftarrow$ c + a\end{lstlisting}
\end{minipage} \hfill $\rightarrow$ \hfill
\begin{minipage}[c]{.35\textwidth}
  \includegraphics[width=\textwidth]{uddu_10b.png}
\end{minipage}

\subsection{La funzione $\Phi$}

\noindent\begin{minipage}[c]{.65\textwidth}
  \begin{itemize}
    \item $\Phi$ fonde multiple definizioni derivanti da multipli percorsi in una singola definizione
    \item per un BB con \textit{p} predecessori ci sono \textit{p} argomenti nella funzione $\Phi$:
      \begin{lstlisting}
xnew $\leftarrow \Phi$(x1, x2, ..., xp)\end{lstlisting}
    \item tipicamente \textbf{non ci interessa quale \lstinline|xi| usare} $\rightarrow$ se rilevante, usiamo le definizioni derivanti dall'arco di interesse
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
  \includegraphics[width=\textwidth]{uddu_11.png}
\end{minipage}

\begin{example}[frametitle={Come si implementa $\Phi$}]
  \noindent \begin{minipage}[c]{.5\textwidth}
    Di fatto, a tempo di esecuzione ci sar\`a sempre un percorso \textbf{effettivamente eseguito} $\rightarrow$ per questo consideriamo $\Phi$ come notazione fittizia
  \end{minipage}\hfill
  \begin{minipage}[c]{.25\textwidth}
    \includegraphics[width=\textwidth]{uddu_12.png}
  \end{minipage}
\end{example}

\subsection{SSA triviale}

\noindent \begin{minipage}[c]{.4\textwidth}
  \begin{itemize}
    \item ogni assegnamento genera una nuova versione della variabile
    \item aggiungiamo una fz.~$\Phi$ \textbf{ad ogni p.to di join $\forall$ le variabili \textit{live}}  
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.55\textwidth}
  \centering
  \includegraphics[width=.7\textwidth]{uddu_13.png}
  \captionof{figure}{Molte di queste funzioni $\Phi$ non sono necessarie}
\end{minipage}

\subsection{SSA minimale}

\noindent \begin{minipage}[c]{.6\textwidth}
  \begin{itemize}
    \item ogni assegnamento genera una nuova versione della variabile
    \item aggiungiamo una fz.~$\Phi$ ad ogni p.to di join $\forall$ le variabili \textit{live} \textbf{con definizioni multiple} 
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.25\textwidth}
  \includegraphics[width=\textwidth]{uddu_14.png}
\end{minipage}

\begin{example}[frametitle={Esempio con loop}]
  \noindent\begin{minipage}[c]{.5\textwidth}
    \lstinline|c1| altro non \`e che la rappresentazione del valore iniziale che assume \lstinline|c| a tempo del primo assegnamento (prima esecuzione del loop), mentre \lstinline|c2| il valore della variabile all'iterazione precedente
  \end{minipage}\hfill
  \begin{minipage}[c]{.45\textwidth}
    \includegraphics[width=\textwidth]{uddu_15.png}
  \end{minipage}
\end{example}

\subsection{Dove inserire le funzioni $\Phi$}

\noindent \begin{minipage}[c]{.7\textwidth}
  Inserisco una $\Phi$ per una variabile \lstinline|a| nel blocco \textit{Z} $\iff$:
  \begin{itemize}
    \item \lstinline|a| \`e stata definita pi\`u di una volta (es.~in \textit{X} e \textit{Y}, $X\neq Y$)
    \item $\exists$ due percorsi da \textit{X} a \textit{Z} e da \textit{Y} a \textit{Z} t.c.
      \begin{itemize}
        \item $Pxz \cap Pyz = \left\lbrace Z \right\rbrace$ (\textbf{\textit{Z} unico BB comune} tra i percorsi)
        \item $Z \notin Pxq \lor Z \notin Pyr,\, \text{dove}\, Pxz = Pxq \rightarrow Z, Pyz = Pyr \rightarrow Z$ (almeno un percorso \textbf{raggiunge \textit{Z} per la prima volta})
      \end{itemize}
  \end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.25\textwidth}
  \includegraphics[width=\textwidth]{uddu_17.png}
\end{minipage}

\begin{emphasize}[frametitle={Note}]
  \begin{itemize}
    \item\textit{ENTRY} contiene una definizione implicita di tutte le variabili
    \item\lstinline|v = |$\Phi$\lstinline|(...)| \`e una definizione di \lstinline|v|
  \end{itemize}
\end{emphasize}

\subsection{Propriet\`a di dominanza della forma SSA}

Nella forma SSA le \textbf{definizioni dominano gli usi}:
\begin{itemize}
  \item se \lstinline|xi| \`e usato in \lstinline|x| $\leftarrow \Phi$ \lstinline|(..., xi, ...)| $\implies$ BB(\lstinline|xi|) \textbf{domina} il predecessore i-esimo di BB($\Phi$)
  \item se \lstinline|x| \`e usato in \lstinline|y| $\leftarrow$ \lstinline|... x ...| $\implies$ BB(\lstinline|x|) \textbf{domina} BB(\lstinline|y|)
\end{itemize}

SI pu\`o usare questa propriet\`a per derivare un algoritmo efficiente per convertire la IR in forma SSA

\subsection{Dominanza e Dominance Frontier}

Risulta interessante considerare i BB "appena prima" o "appena dopo" i blocchi dominati o che ci dominano $\rightarrow$ i \textbf{dominators} e \textbf{postdominators} ci dicono quale BB \textbf{deve} essere eseguito \textbf{prima}, o \textbf{dopo}, un certo BB \textit{x}\\

\noindent\textbf{Dominance Frontier} di un nodo \textit{x}: $\boxed{DF(x) = \left\lbrace w | x\, dom\, pred(w) \land !(x\, sdom\, w)\right\rbrace}$

\noindent $\rightarrow$ ovvero, l'insieme di tutti i BB che sono successori immediati dei blocchi dominati da \textit{x}, ma che non sono strettamente dominati da \textit{x}
\newpage
\begin{example}
  \noindent\begin{minipage}[c]{.5\textwidth}
    Per il CFG di esempio, \underline{$DF(5) = \left\lbrace 4,5,12,13 \right\rbrace$}: osservando il DT (pag.~\pageref{example-dt}), notiamo che 4, 12, 13 sono successori di 6, 7, 8 (prima parte della condizione), e non sono dominati da 5 (dunque neppure strettamente, seconda parte). Anche 5 \`e successore di 8, e poich\'e un nodo non domina strettamente se stesso, lo includiamo nella $DF$
  \end{minipage}
  \begin{minipage}[c]{.5\textwidth}
    \includegraphics[width=\textwidth]{uddu_18.png}
  \end{minipage}
\end{example}

\subsection{Utilizzo della Dominance Frontier per calcolare la forma SSA}

I passaggi da seguire sono
\begin{enumerate}
  \item posizionare tutte le funzioni $\Phi$ (se c'\`e una definizione di \lstinline|a| nel blocco \textit{x}, i nodi nella \textit{DF(x)} \textbf{necessitano di una funzione $\Phi$} per \lstinline|a|)
  \item rinominare tutte le variabili
\end{enumerate}

\subsubsection{Utilizzo della DF per posizionare le funzioni $\Phi()$}

\begin{itemize}
  \item identifico tutti i siti dove vengono definite le variabili 
  \item $\forall$ variabile \lstinline|v|:
    \begin{itemize}
      \item $\forall$ sito di definizione \textit{n} (\textbf{non siamo ancora in forma SSA, la stiamo costruendo}):
        \begin{itemize}
          \item $\forall$ nodo in \textit{DF(n)}
            \begin{itemize}
              \item se non c'\`e una $\Phi()$ nel nodo, mettiamone una
              \item se questo nodo non definiva la variabile in precedenza, aggiungiamo questo nodo alla lista dei siti di definizione
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

\paragraph{Algoritmo}~\\


\begin{lstlisting}[morekeywords={foreach,node,variable,in,remove,from,insert,defined}]
foreach node n {
  foreach variable v defined in n {
  orig[n] $\cup =$ {v}
  defsites[v] $\cup =$ {n}
  }
}
foreach variable v {
  W = defsites[v]
  while W not empty {
    n = remove node from W
    foreach y in DF[n] {
      if y $\notin$ PHI[v] {
        insert "v $\leftarrow \Phi$(v,v,...)" at top of y
        PHI[v] $\cup =$ {y}
        if v $\notin$ orig[y]: W = W $\cup$ {y}
      }
    }
  }
}\end{lstlisting}

Questo algoritmo calcola iterativamente la DF, inserendo il numero minimo necessario di funzioni $\Phi()$

\subsubsection{Rinominare le variabili}

\paragraph{Algoritmo}

\begin{itemize}
  \item scorrere il DT, rinominando le variabili come le si incontra
  \item rimpiazzare gli usi con la pi\`u recente definizione rinominata
  \item in presenza di branch e join: usare la definizione pi\`u vicina tale per cui la definizione si trova \textbf{prima dell'uso} nel DT
\end{itemize}

\paragraph{Algoritmo per implementazione tramite stack}~\\

\begin{lstlisting}[morekeywords={foreach,statement,node,variable,in,remove,from,insert,defined,replace,define,push,onto,by}]
// inizializza stacks e contatori
foreach variable v {
  stacks[v] = $\emptyset$
  counters[v] = 0
}

// procedura per la generazione del prossimo nome
define GenName(variable v) {
  i = counters[v]
  replace v by v$_i$
  push i onto stacks[v]
  counters[v]++
}

// procedura per il renaming visitando in pre-order il DT
define Rename (node X) {
  foreach $\Phi$ p in X { // scorro i phi
    GenName(LHS(p)) // nuova def.
  } 

  foreach statement a in X { // scorro il resto degli statement
    foreach variable v in RHS(a) {
      i = top(stacks([v])
      replace v by v$_i$ // sostituisco gli usi con la def. piu' recente
    }
    foreach variable v in LHS(a): GenName(v) // nuova def.
  }

  foreach Y in succ(X) { // sostituisco gli usi nelle phi dei successori
    j = "position in Y's $\Phi$ corresponding to X"
    foreach $\Phi$ p in Y { 
      v$_k$ = LHS(p) // k fittizio, utile solo per recuperare stacks[v]
      i = top(stacks[v])
      replace RHS(p)[j] by v$_i$
    }
  }

  foreach Y in succ(X) { // visito ricorsivamente i successori
    Rename(Y)
  }

  foreach $\Phi$ or statement a in X { // pop delle var. definite in X
    foreach v$_i$ in LHS(a): pop(stacks[v])
  }
}\end{lstlisting}

\section{Ottimizzazioni sulla memoria}

\subsection{Memorie, tecnologie RAM}

Le ottimizzazioni sulla memoria sono solitamente \textbf{architecture dependent} (si effettuano nel backend), ma alcune si possono gi\`a fare nel middle end, conoscendo le \textbf{problematiche generali di efficienza} della DRAM (memoria principale)

\subsubsection{Tecnologia DRAM}

\begin{itemize}
  \item Dati immagazzinati in condensatori, in cui c'\`e o non c'\`e carica elettrica; va effettuata periodicamente una operazione di \textit{refresh}
  \item bit in DRAM organizzati come \textbf{array rettangolari}
  \item accesso effettuato su righe: insieme di condensatori lungo un certo numero di \textit{words}
  \item \textbf{accessi costosi}: cerco di minimizzarli, e in particolare massimizzare quelli nella riga attuale 
  \item negli anni si sono studiate varie possibili ottimizzazioni, in particolare sulle modalit\`a di accesso
    \begin{itemize}
      \item \textbf{burst mode}: latenza ridotta per accessi a parole successive nella stessa riga
      \item \textbf{DDR} (Double Data Rate) RAM: sfrutta i fronti di salita e discesa del clock del circuito elettronico come momenti di trasferimento dati
      \item \textbf{QDR} (Quad DR): separa input e output di DDR
    \end{itemize}

  \item[$\rightarrow$] sono tutti trucchi per migliorare i tempi delle operazioni su HW che di base \`e "lento"
\end{itemize}

\paragraph{DRAM generations}~\\

Negli anni la tecnologia DRAM si \`e dimostrata estremamente \textbf{cost effective} (costo/unit\`a di memoria), mentre la \textbf{latenza} di accesso (sia a righe nuove che quelle gi\`a caricate) decresce molto pi\`u lentamente $\rightarrow$ scarsa in termini di performance

\subsubsection{DRAM performance factors}

Vediamo altre tecniche per migliorare le prestazioni:
\begin{itemize}
  \item \textbf{row buffer}: mantengo un buffer delle righe accedute di frequente
  \item \textbf{DRAM sincrona}: sfrutta accessi in burst per migliorare la banda (quantita di mem acceduta o trasferita in un periodo di tempo)
  \item \textbf{DRAM banking}: 
    \begin{itemize}
      \item \textit{master}: qualsiasi device (CPU, periferiche, ecc.) che pu\`o essere utilizzatore della memoria
      \item se tanti master devono accedervi (gia \`e lenta), la DRAM diventa bottleneck per le prestazioni
      \item il concetto di banking sfrutta quello di parallelismo: ho piu "banchi" di DRAM che operano \textbf{indipendentemente} tra loro
      \item abbasso il \textbf{tempo medio} di accesso: i master accederanno a risorse che non necessariamente si trovano sullo stesso banco
    \end{itemize}  
\end{itemize}

\subsubsection{Memory wall}

Uno dei "muri" tecnologici che hanno portato alla rivoluzione dei primi 2000 sul passaggio da single a multicore CPU
\begin{itemize}
  \item Cresce moltissimo il divario tra CPU e DRAM performance (\textbf{55\% aumento annuo vs 7\%})
  \item $CPI_{\text{ideale}} \leq 1$, mentre \lstinline|load| e \lstinline|store| (uniche a interagire con DRAM) hanno $CPI_{l/s} \simeq 100$
  \item naturalmente $\nexists$ programmi che non accedono alla memoria, e negli anni 80 non ci si poneva questi problemi ($\nexists$ cache, il costo di op.~su DRAM era equiv.~a quello di op.~su CPU)
  \item[$\rightarrow$] cambio di approccio: si introducono le cache
\end{itemize}

\subsection{Cache}

\begin{itemize}
  \item Usano tecnologia SRAM (Static R.), basata su transistor che decide se far passare o meno corrente
  \item si trovano sulla motherboard \textbf{molto vicino alla CPU}: anche questo aumenta la velocit\`a 
  \item[$\rightarrow$] \textbf{t.~di accesso} molto ridotti (ordine dei \textit{ns}, freq.~di GHz - come la CPU) e \textbf{densit\`a} molto pi\`u alta
  \item ovviamente, a discapito del costo $\rightarrow$ per questo usiamo le \textbf{gerarchie di memoria}
\end{itemize}

\subsubsection{Gerarchie di memoria}
\begin{itemize}
  \item \textbf{block} (\textbf{line}): unit\`a di copia, solitam.~multiple \textit{word}
  \item in base alla presenza o meno del dato nell'attuale livello di cache, ottengo una \textbf{hit} o una \textbf{miss} - quando \textit{misso} devo fermare la loadstore unit e copiare il dato dalla cache inferiore (e cos\`i via)
  \begin{itemize}
    \item hit ratio: $hits\over accesses$
    \item miss penalty: tempo necessario per gestire la miss
    \item miss ratio: $1 - hit\,ratio$
  \end{itemize}
\end{itemize}

  \begin{example}[frametitle={Velocit\`a delle cache}]
    \begin{itemize}
      \item L1: 1-2 \textit{cicli per accesso} ($=$CPU), ma molto piccola
      \item L2 $\sim$ 5-10 (nonostante stessa tecnologia): in parte per dimensioni maggiori, ma soprattutto perch\'e devo \textbf{spezzare il critical path}
      \item \textbf{c.~path}: path fisico pi\`u lungo nel circuito - il clock rate deve adattarvisi (CPU monolitiche devono garantire l'esecuzione di tutti gli stadi di un'istr.~in 1 ciclo), dunque per \textit{cp} $> \implies $ freq.~$<$
      \item scelta di design basilare: spezzare il critical path verso L2 con dei registri intermedi $\rightarrow$ aumenta il \textit{CPaccess} ma non penalizza il clock r.
    \end{itemize}
  \end{example}

\subsubsection{Principi di localit\`a}

Si basano sulla conoscenza generale di memoria e cache (HW), e sui tipici pattern di accesso ai dati (a liv.~SW)

\begin{itemize}
  \item \textbf{l.~temporale}: el.~acceduti recentemente sono probabilmente necessari di nuovo in breve (es.~istr.~in loop)
  \item \textbf{l.~spaziale}: probabilit\`a di accedere in successione a el.~contigui (es.~accesso ad array)
\end{itemize}
\begin{example}[frametitle={Visualizzazione grafica delle localit\`a}]
 
  \noindent\begin{minipage}[c]{.5\textwidth}
  \begin{itemize}
    \item no locality: segmenti in cui non ho accessi o non ho pattern di accesso
    \item temporal locality: righe dritte orizzontali
    \item spatial locality: righe diagonali
  \end{itemize}
  \end{minipage}
  \hfill
  \begin{minipage}[c]{.5\textwidth}
  \includegraphics[width=\textwidth]{mem_2.png}
  \end{minipage}
\end{example}

\subsubsection{Cache misses}

\begin{itemize}
  \item \textbf{cold cache} (\textbf{compulsory}): non ci sono dati nella cella della cache (tipicamente appena eseguo il programma)
  \item \textbf{conflict}: quando due indirizzi in DRAM sono mappati alla stessa linea di cache
  \item \textbf{capacity}: ho finito lo spazio disponibile in cache
\end{itemize}

\subsubsection{Tipologie di cache}

\paragraph{Direct mapped}

\begin{itemize}
  \item linee mappate in base agli ultimi bit dell'indirizzo in DRAM
  \item \textit{valid bit} garantisce la validit\`a di una linea
  \item schema di indirizzamento e rete logica semplice (area, costo, velocit\`a $<$) $\rightarrow$ \textbf{\textit{hit time} basso}
  \item uso non ottimale dello spazio: frequenti \textit{conflict} (1 sola possibile locazione in cache $\forall$ linea)
\end{itemize}

\paragraph{Fully associative}

\begin{itemize}
  \item nessun indirizzamento fisso, la nuova entry occupa la prossima linea libera
  \item richiede la ricerca \textbf{in tutte le linee} contemporan.~$\rightarrow$ \textbf{costoso} (1 comparatore $\forall$ entry)
\end{itemize}

\paragraph{N-way set associative}

\begin{itemize}
  \item mantiene il concetto di bit di indice, per indicare non un indirizzo unico ma un \textbf{set} di indirizzi, e poi tanti comparatori quanti gli elementi del set 
\end{itemize}

\noindent
\begin{minipage}[b]{.4\textwidth}
  \includegraphics[width=\textwidth]{mem_3.png}
  \captionof{figure}{Cache direct mapped}
\end{minipage}
\hfill
\begin{minipage}[b]{.5\textwidth}
  \includegraphics[width=\textwidth]{mem_4.png}
  \captionof{figure}{Le tre tipologie di cache}
\end{minipage}

  \begin{emphasize}
      Per sfruttare la spatial l.~le linee di cache devono essere pi\`u grandi di una word (in seguito a una miss carico molti dati adiacenti e riduco miss rate), ma per cache di dim.~fissa blocchi molto grandi aumentano la competizione e inquinano la cache
  \end{emphasize}

\subsection{Memory Hierarchy optimizations}

Queste ottimizzazioni vogliono
\begin{itemize}
  \item \textbf{ridurre la latenza di accesso} alla memoria e diminuire CPI per \lstinline|load| e \lstinline|store|
  \item \textbf{massimizzare la banda} di memoria: poter accedere a pi\`u dati nella stessa unit\`a di tempo (stesso concetto della pipeline: non riduco il tempo di esec.~di una singola istruzione, ma a regime aumento il throughput) 
  \item \textbf{gestire overhead}, i costi addizionali dovuti all'ottimizzazione: maschero il tempo "perso" sovrapponendo altre operazioni
\end{itemize}

\subsection{Reuse and locality}

Consideriamo come accediamo ai dati:
\begin{itemize}
  \item \textbf{data reuse}: sfrutta la localit\`a temporale, dunque il riuso di degli stessi dati o molto vicini tra loro - posso assicurare che il dato acceduto rimanga sempre in cache
  \item \textbf{data locality}: (es.~accesso ad array) il compilatore si deve assicurare che questi dati vengano acceduti in questo modo, e poi garantire la loro presenza
\end{itemize}
\begin{emphasize}
  Le miss dipendono dai pattern di accesso, dunque anche dal comportamento dell'algoritmo e delle ottimizzazioni del compilatore
\end{emphasize}

\subsection{Optimizing cache performance}
\label{cache-opt}

Se una computazione ha del riuso, posso ottimizzarlo tramite
\begin{itemize}
  \item layout e posizionamento ottimale dei dati
  \item trasformazioni sul codice (\textit{reordering} delle operazioni)
\end{itemize}

\noindent
\begin{minipage}[t]{.45\textwidth}
\paragraph{Cosa migliorare}
\begin{itemize}
  \item l.~spaziale
  \item l.~temporale
\end{itemize}
\end{minipage}\hfill\vline\hfill
\begin{minipage}[t]{.45\textwidth}
\paragraph{Cosa minimizzare}
\begin{itemize}
  \item conflitti (es.~rimpiazzamenti sbagliati)
\end{itemize}
\end{minipage}

\subsubsection{Il ruolo del compilatore}

Con il compilatore possiamo ottimizzare
\begin{itemize}
  \item \textbf{tempo} (\textit{quando accedo a un dato?}) $\rightarrow$ riordino delle operazioni
  \begin{itemize}
    \item \textit{cosa rende difficile sapere \textbf{quando} accedo ad un oggetto?}
    \item \textit{come possiamo predire un momento di accesso \textbf{migliore}?}
    \item \textit{come possiamo essere sicuri che il riordino sia \textbf{safe}?}
  \end{itemize}
  
  \item \textbf{spazio} (\textit{dove esiste il dato nell'address space?}) $\rightarrow$ modifica del layout dei dati
  \begin{itemize}
    \item \textit{cosa sappiamo della \textbf{locazione} del dato? (dipende dal tipo)}
    \item \textit{come possiamo stabilire un layout \textbf{migliore} e quanto possiamo \textbf{alterare quello attuale}?}
  \end{itemize}
  
\end{itemize}


\subsection{Tipi di oggetti da considerare}

\begin{itemize}
  \item scalari: tipi di dato semplici, non aggregati
  \item strutture dati e puntatori (piu complessi)
  \item array
\end{itemize}

\subsubsection{Scalari}

\noindent\begin{minipage}[c]{.6\textwidth}
\begin{itemize}
  \item var.~\textcolor{ForestGreen}{\textbf{locali}} (scope e tempo di vita locale alla funzione in cui sono definite)
  \item var.~\textcolor{BurntOrange}{\textbf{globali}} (scope e tempo di vita globali pari alla vita del programma)
  \item \textcolor{Cyan}{\textbf{argomenti}} delle funzioni
\end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
\begin{lstlisting}
int $\color{BurntOrange}\texttt{x}$;
double $\color{BurntOrange}\texttt{y}$;
foo(int $\color{Cyan}\texttt{a}$){
  int $\color{ForestGreen}\texttt{i}$;
  $\ldots$
  $\color{BurntOrange}\texttt{x}$ = $\color{Cyan}\texttt{a}$*$\color{ForestGreen}\texttt{i}$;
  $\ldots$
}\end{lstlisting}
\end{minipage}


Tipicamente sono gestiti sempre tramite registri $\rightarrow$ non dobbiamo gestire gli accessi alla memoria (salto la fase di MEM, e la load-store unit non esegue nulla per questa istruzione)

\begin{emphasize}
  Unico caso problematico: finiscono i registri utilizzabili $\rightarrow$ devo fare uno spill sulla memoria, in particolare sullo stack frame (ovvero faccio una store, e poi una load , esattamente come per i save registers (modificati da una chiamata a funzione, che alla fine dell'esecuzione deve ripristinarli)
\end{emphasize}

$\rightarrow$ \textbf{ottimizzazioni di backend} (calling convention, register file, ISA sono HW specific)

\subsubsection{Strutture dati e puntatori}

\noindent\begin{minipage}[c]{.6\textwidth}
  \begin{itemize}
    \item struct: di base un'\textbf{aggregazione di bytes} $\rightarrow$ elem.~\textbf{contigui in memoria} $\rightarrow$ in generale un oggetto complesso potrebbe non essere grande un multiplo di word o cache line, dunque \textbf{non posso sfruttare la spatial locality}
    \item posso \textbf{trasformare} la struttura del dato, ad esempio \textbf{estraendo dati di tipi primitivi} (nel caso di array di tipi composti) e costruendo array separati (devo conoscere la struttura della str.~dati)
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.35\textwidth}
\begin{lstlisting}
struct {
  int count;
  double vel;
  double inertia;
  struct node *neighb[N];
} node;\end{lstlisting}
\end{minipage}


\subsubsection{Array}


\noindent\begin{minipage}[c]{.6\textwidth}
\begin{itemize}
  \item posso ottimizzarli sfruttando principi di l.~spaziale
  \item solitamente accesso tramite loop nests (array multidim., op.~matriciali, ...)
  \item staticamente, possiamo analizzare l'andamento della posizione relativa dell'elemento acceduto, ovvero l'evoluzione dell'\textbf{iteration space}

\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.35\textwidth}
\begin{lstlisting}
double A[N][N], B[N][N];
$\ldots$
for i = 0 to N-1
  for j = 0 to N-1
    A[i][j] = B[j][i];\end{lstlisting}

\end{minipage}

\subsection{Iteration space}

\noindent\begin{minipage}[c]{.6\textwidth}
\begin{lstlisting}
for $\color{blue}\texttt{i}$ = 0 to N-1
  for $\color{red}\texttt{j}$ = 0 to N-1
    A[i][j] = B[j][i];\end{lstlisting}

\begin{itemize}
  \item ogni posizione rappresenta un'iterazione
  \item \textbf{iteration s.~$\neq$ data s.}
\end{itemize}
\end{minipage}
\hfill
\begin{minipage}[c]{.3\textwidth}
\includegraphics[width=\textwidth]{mem_5.png}
\end{minipage}

\begin{example}[frametitle={Dall'esempio: cache miss per accessi ad \lstinline|A|}]

  \noindent\begin{minipage}[c]{.7\textwidth}
  Immaginiamo una cache di 4 parole per riga e 8 righe, vuota in partenza
  \begin{itemize}
    \item \lstinline|A[0][0]|: \textit{cold cache} m.
    \item \lstinline|A[0][1]-A[0][3]|: \textit{hit} avendo caricato l'intera riga
    \item si ripete fino a \lstinline|A[0][15]|
    \item a partire da \lstinline|A[1][0]|: \textit{conflict} m.~(i bit di indexing si sovrappongono)
  \end{itemize}
  \textbf{NB}: non cambia il pattern di miss (1 ogni 4)
  \end{minipage}
  \begin{minipage}[c]{.3\textwidth}
    \centering
  \includegraphics[width=.65\textwidth]{mem_6.png}
  \includegraphics[width=.65\textwidth]{mem_7.png}
  \end{minipage}
\end{example}

\begin{example}[frametitle={Dall'esempio: cache miss per accessi ad \lstinline|B|}]

  \noindent\begin{minipage}[c]{.7\textwidth}
    \begin{itemize}
      \item \lstinline|B[0][0]-B[3][0]| \textit{cold cache}, seguono \textit{conflict} (suppongo stesso indexing per $1^{a}$ e $5^{a}$ riga)
      \item \lstinline|B[0][1]|: \textbf{conflict} $\rightarrow$ la linea attualmente caricata \`e quella con \lstinline|B[3][1]|
      \item situazione di \textbf{miss sistematica}, rilevabile e da correggere
      \item possiamo distinguere accessi \textbf{cache friendly} e non \underline{anche} dalla discordanza tra \textit{iteration} e \textit{data} s.
    \end{itemize}
  \end{minipage}
  \begin{minipage}[c]{.3\textwidth}
    \centering
  \includegraphics[width=.65\textwidth]{mem_8.png}
  \includegraphics[width=.65\textwidth]{mem_9.png}
  \end{minipage}
\end{example}

\subsection{Cache behavior of array accesses}

Per ottimizzare gli accessi ci chiediamo 
\begin{itemize}
  \item \textbf{quando avvengono} le miss? $\rightarrow$ \textit{locality analysis}
  \item possiamo cambiare \textbf{ordine delle iterazioni (o data layout)} per migliorare il comportamento degli accessi? 
    \begin{itemize}
      \item valuto il \textbf{costo} delle alternative, per stimare il beneficio ottenuto $\rightarrow$ giustificato solo se il riuso \textbf{copre il costo aggiuntivo} (es.: precalcolo di una matrice trasposta) 
    \end{itemize}
  \item ottengo sempre \textbf{risultati corretti}? $\rightarrow$ verifico gli effetti "collaterali" (\textit{dependence analysis})
    \begin{itemize}
      \item dipendenza: \textbf{vincolo nell'ordine di esecuzione delle istr.}, dunque nell'iteration space
    \item caso di loop-carried dependency semplice: \lstinline|A[i] = A[i+1]|
      \item framework di analisi solitamente basati sul \textit{polyhedral model}
    \end{itemize}
\end{itemize}

\begin{emphasize}
  Sono ottimizzazioni di tipo "best effort" basate su euristiche $\rightarrow$ ottimizzano l'esec.~del caso \textbf{medio}, e nel caso peggiore non ottimizzano nulla ma \textbf{non peggiorano l'esecuzione}
\end{emphasize}

\subsubsection{Approcci per variabili di cui non conosco il valore staticamente}
\begin{itemize}
    \item \textbf{conservativo}: non so determinare il valore, dunque (pessimisticamente) non faccio nulla
    \item genero entrambe le versioni del codice, e \textbf{stabilisco a runtime quale eseguire} (aggiungo un condizionale che esegue il pr.~ottimizzato solo per qualche \lstinline|N| abbastanza grande da giustificare il costo aggiuntivo)
  \end{itemize}


\subsection{Esempi di trasformazioni per i loop}

\subsubsection{Loop interchange}

\noindent\begin{minipage}[c]{.45\textwidth}
\textbf{Scambio} (o permuto) l'ordine dei loop: in assenza di dipendenze, non altero in nessun modo il risultato finale e \textbf{miglioro la performance} (magari non accedendo nello stesso ordine di prima, ma se indipendente non importa)
\end{minipage}\hfill
\begin{minipage}[c]{.5\textwidth}
\includegraphics[width=\textwidth]{mem_10.png}
\end{minipage}


\paragraph{Obiettivi della permutazione}

\begin{itemize}
  \item locality optimization (specialmente quella spaziale)
  \item riarrangiare i loop nest per ottenere la giusta \textbf{granularit\`a} del parallelismo:
    \begin{itemize}
      \item verso l'interno: parallelismo \textit{fine-grain} $\rightarrow$ frammento il lavoro da svolgere in blocchi pi\`u piccoli, e mantengo pi\`u stabile la q.t\`a di lavoro tra thread e proc.
      \begin{emphasize}
          Caso estremo: task talmente piccoli che l'overhead (costo per creare thread, assegnare lavoro, proc.~sync) supera il tempo di esecuzione (tipicamente il costo di creazione di un thread ha costo fisso $\rightarrow$ \textbf{dipende da HW})
      \end{emphasize}
      \item esterno: \textit{coarse-grain} parallelism
        \begin{emphasize}
        Caso estremo: pochi task rispetto ai proc.~disponibili: se un proc.~si ritrova un blocco "lento" (es.~molte miss), gli altri dovranno aspettarlo una volta finito il loro lavoro
        \end{emphasize}
    \end{itemize}
\end{itemize}

\subsubsection{Loop tiling/blocking}

\begin{emphasize-blue}
  In generale, il tiling viene usato gerarchicamente a "tutti" i livelli di memoria (registri, caches, buffer SW-managed, memorie piccole, ...) per gestire limiti di spazio
\end{emphasize-blue}
\begin{example}[frametitle={Esempio: Matrix multiplication}]

  \noindent \begin{minipage}[]{.5\textwidth}
  \begin{lstlisting}
for i = 0 to N-1
  for j = 0 to N-1
    for k = 0 to N-1
      C[i][j] += A[i][k] * B[k][j]\end{lstlisting}
  \end{minipage}\hfill
  \begin{minipage}[c]{.4\textwidth}
  \includegraphics[width=\textwidth]{mem_13.png}
  \end{minipage}
  
  Consideriamo il caso in cui non posso ospitare l'intera struttura in cache
  \begin{itemize}
    \item 3 matrici \lstinline|A,B,C| $N\times N$, con $c_{ij}=\sum_{k=1}^{N}a_{ik}b_{kj},\, i,j=1,\ldots,N$
    \item 3 loop innestati, di cui i primi due iterano sugli el.~di \lstinline|C|, e il terzo scorre sulla riga e colonna corrispondenti
    \item supponiamo che $N > dim(line_{cache})$ $\rightarrow$ linee multiple per una riga intera
    \item ogni riga di A viene usata $\forall$ colonna di B (e viceversa, ma iterare su colonne \`e meno \textit{cache friendly}) $\rightarrow$ conviene tenerla in cache
\end{itemize}

  Nel loop pi\`u interno:
  \begin{itemize}
    \item \lstinline|C[i][j]| acceduto 1 volta, nessun riuso
    \item \lstinline|A[i][k]| comporta accesso all'intera riga (che non sta in line singola)
    \item \lstinline|B[k][j]| comporta accesso all'intera colonna (non sta in cache e ha bassa \textit{spatial l.})
  \end{itemize}
  Miss totali - $\boxed{2N^3 + N^2}$ :
  \begin{itemize}
    \item \lstinline|C|: $N^{2}$: ogni el.~viene scritto $N$ volte in sequenza (ma rimane in cache)
    \item \lstinline|A|: $N^{3}$: riga letta $N$ volte in sequenza (ma non resta in cache)
    \item \lstinline|B|: $N^{3}$: col.~letta $N$ volte in sequenza (non resta e accedo in \textit{column-major} order)
  \end{itemize}

  \begin{itemize}
    \item[$\rightarrow$] cerco una dim.~ottimale di blocco che permette di \textbf{avvicinare temporalmente il riuso dei dati}
    \item cambiare l'ordine delle iterazioni e iterare "a blocchi" permette di sfruttare al massimo il riuso dei dati (altrimenti siamo sicuri che gli usi sono talmente lontani da causare \textit{conflict})
    \item[$\rightarrow$] aggiungo dei loop nest, che permettono di iterare blocco per blocco invece che a dimensione intera
  \end{itemize}

\begin{center}
  \begin{minipage}[c]{.4\textwidth}
  \includegraphics[width=\textwidth]{mem_14.png}
  \end{minipage}
  \begin{minipage}[c]{.4\textwidth}
  \includegraphics[width=\textwidth]{mem_15.png}
  \end{minipage}
  
\end{center}


\end{example}

\subsection{Loop fusion}

\noindent\begin{minipage}[c]{.65\textwidth}
Obiettivo: "fondere" tra loro loop separati del programma che accedono alle stesse strutture, per \textbf{garantire il riuso dei dati caricati in cache} (annullo la distanza tra gli usi degli stessi dati)

\begin{emphasize}
    Quando non conviene ottimizzare? Quando sono sicuro che la somma delle dimensioni dei dati (le matrici in questo caso) \`e \textbf{minore della dim.~della cache} (i dati saranno ancora disponibili)
\end{emphasize}
\end{minipage}\hfill
\begin{minipage}[c]{.3\textwidth}
  \centering
\includegraphics[width=\textwidth]{mem_11.png}
$\downarrow$
\includegraphics[width=\textwidth]{mem_12.png}
\end{minipage}

\subsection{Software prefetching}

\subsubsection{Coping with memory latency}

\begin{itemize}
  \item \textbf{ridurre la l.}: \textit{locality optimizations} (sez.~\ref{cache-opt})  - riordino iterazioni e layout per migliorare il riuso
  \item \textbf{tollerare la l.}: \textit{SW prefetching} - carico i dati in cache prima dell'uso per evitare stalli della load-store u. (comportamento \textbf{non lazy})
\end{itemize}

\subsubsection{Tollerare la latenza con il prefetching}

I sistemi HW moderni permettono di disaccoppiare e \textbf{sovrapporre la computazione con gli accessi in memoria} $\rightarrow$ nascondo le latenze "infilando" il prefetching sotto ad altre operazioni

\vspace{.5em}
\noindent\begin{minipage}[b]{.5\textwidth}
\begin{itemize}
  \item Posso eseguire il pref.~sia come ottimizzazione che direttamente nel sorgente
  \item un'istr.~di prefetching agisce come una \lstinline|load| non bloccante
\end{itemize}
\end{minipage}\hfill
\begin{minipage}[c]{.4\textwidth}
\includegraphics[width=\textwidth]{mem_16.png}
\end{minipage}

\begin{emphasize}[frametitle={Problemi pratici nel calcolo del prefetching}]
    Nella teoria sono "certo" di quando ho bisogno di un dato, mentre in pratica \textbf{le risorse sono condivise} (possibili conflitti di accesso tra processi)
    \begin{itemize}
      \item pref.~\textbf{troppo in anticipo} $\implies$ la mem.~potrebbe essere sovrascritta
      \item pref.~\textbf{"tardi"} $\implies$ inutile
    \end{itemize}
    
\end{emphasize}

\subsubsection{Algoritmo di prefetching del compilatore}

Tipicamente lavoro su \textbf{loop} (sono strutture ad esec.~regolare), e svolgo
\begin{itemize}
  \item \textbf{analisi} (cosa prefetchare) $\rightarrow$ \textit{analisi di localit\`a}
  \item \textbf{scheduling} (quando/come eseguirlo) $\rightarrow$ \textit{loop splitting} e \textit{SW pipelining}
\end{itemize}

\paragraph{Loop splitting}~\\

  \textbf{Decompongo i loop} per \textbf{isolare istanze di \textit{cache miss}} (meno costoso di inserire \lstinline|if|)

\begin{center}
  \includegraphics[width=.45\textwidth]{mem_17.png}
\end{center}

Applico ricorsivam.~per loop \textbf{innestati}, sopprimo per loop \textbf{troppo grandi} (evito \textit{code explosion})

\paragraph{Sofware pipelining}~\\

\begin{minipage}[c]{.55\textwidth}
Data la regolarit\`a dei loop, posso esprimere la \textbf{distanza di pre-fetch} come un n$^{o}$ di loro it.:

\begin{example}[frametitle={}]
    \begin{itemize}
      \item \textit{l}: mem.~latency (tempo di lettura)
      \item \textit{s}: shortest path attraverso il loop body
      \item [$\implies$] \textbf{it.~necessarie per coprire il pref.}~$It = \left\lceil {l\over s} \right\rceil$
    \end{itemize}
\end{example}

A questo punto suddivido il loop in 3 parti: 
\begin{itemize}
  \item \textbf{prolog}: solo prefetch, \textit{It} volte (es.~$It=5$)
  \item \textbf{steady state} (s.~a regime): pref.~e accedo ai dati pronti
  \item \textbf{epilog}: finisco di usare gli ultimi dati prefetchati
\end{itemize}

\begin{emphasize}
  Sono ottimizzaz.~che si basano sempre su euristiche \textbf{best effort} $\rightarrow$ ovviamente \textbf{non posso eseguirle su feature e sistemi ritenuti critici}
\end{emphasize}
\end{minipage}
\hfill
\begin{minipage}[c]{.35\textwidth}
  \centering
\begin{lstlisting}
for (i = 0; i<100; i++) {
  a[i] = 0;
}\end{lstlisting}
$\Downarrow$
\begin{lstlisting}
/* Prolog */
for (i = 0; i<5; i++) {
  prefetch(&a[i]);
}
/* Steady State */
for (i = 0; i<95; i++) {
  prefetch(&a[i+5]);
  a[i] = 0;
}
/* Epilog */
for (i = 95; i<100; i++) {
  a[i] = 0;
}\end{lstlisting}
\end{minipage}

\end{document}
